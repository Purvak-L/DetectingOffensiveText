{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pprint\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import string\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "from gensim import corpora, models\n",
    "from keras import layers, models, optimizers\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "#preprocessing pipeline\n",
    "#Pipeline models features like word count, tfidf, word density, word embeddings (GloVe)\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from keras.preprocessing import text, sequence\n",
    "import xgboost\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = pd.read_csv('hatespeech.csv', encoding=\"ISO-8859-1\",index_col=6, keep_default_na=False)\n",
    "#print(hs.head())\n",
    "\n",
    "orig = pd.read_csv('NAACL_SRW_2016.csv', index_col=0, header=None)\n",
    "orig.index.name = 'ID'\n",
    "orig = orig.rename(columns={1: 'Class'})\n",
    "orig.index = orig.index.astype(str)\n",
    "#print(orig.head())\n",
    "\n",
    "#merging the two dataframes\n",
    "hs = pd.merge(hs, orig, how='inner', left_index=True, right_index=True)\n",
    "#print(hs.head())\n",
    "df = hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Descriptions</th>\n",
       "      <th>Favorite Count</th>\n",
       "      <th>Follower Count</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Locations</th>\n",
       "      <th>Time Tweeted</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>User Mentions</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>319399851215433729</th>\n",
       "      <td>2004</td>\n",
       "      <td>CreatrixKali</td>\n",
       "      <td>Literary Creatrix for Alternative SpiritMag. Composer of articles of quirk and sometimes even remotely interesting status updates. And I'm in a book! Link below</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "      <td>[{'text': 'MKR', 'indices': [37, 41]}]</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2013-04-03 10:43:53</td>\n",
       "      <td>Oh yeah Colin! Smash those girls! :D #MKR</td>\n",
       "      <td>[]</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320817818222358529</th>\n",
       "      <td>5148</td>\n",
       "      <td>quincepoacher</td>\n",
       "      <td>AKA Queenotisblue. Sociology, politics, policy, food, Gen Xer, Coburger &amp; grumpy old lady. Social justice, education &amp; employment stuff.</td>\n",
       "      <td>0</td>\n",
       "      <td>425</td>\n",
       "      <td>[{'text': 'MKR', 'indices': [68, 72]}]</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>2013-04-07 08:38:23</td>\n",
       "      <td>It's insane they keep bringing people back. When will this show end #MKR</td>\n",
       "      <td>[]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324114200450437120</th>\n",
       "      <td>5167</td>\n",
       "      <td>MarkTramby</td>\n",
       "      <td>Mad keen Manly Sea Eagles supporter, love red wine (the better stuff), cricket ( all forms) and my wonderful family</td>\n",
       "      <td>0</td>\n",
       "      <td>330</td>\n",
       "      <td>[{'text': 'MKR', 'indices': [16, 20]}, {'text': 'TheVoiceAu', 'indices': [92, 103]}]</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>2013-04-16 10:57:01</td>\n",
       "      <td>@berkeley_eagle #MKR  this shit show has more comebacks than Johnny Farnham, ok back to the #TheVoiceAu</td>\n",
       "      <td>[{'screen_name': 'berkeley_eagle', 'name': 'john G', 'id': 33840083, 'id_str': '33840083', 'indices': [0, 15]}]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326286656854454273</th>\n",
       "      <td>5221</td>\n",
       "      <td>BinnyD</td>\n",
       "      <td>Reclusive nillionaire Whovian who enjoys television, arts, crafts and poking dead things with a stick.</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>[{'text': 'MKR', 'indices': [18, 22]}]</td>\n",
       "      <td>NSW, Australia</td>\n",
       "      <td>2013-04-22 10:49:35</td>\n",
       "      <td>*sigh* oh Colin ? #MKR</td>\n",
       "      <td>[]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381988216292655104</th>\n",
       "      <td>3612</td>\n",
       "      <td>YesYoureSexist</td>\n",
       "      <td>Inspired by @YesYoureRacist.</td>\n",
       "      <td>0</td>\n",
       "      <td>24041</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>2013-09-23 03:47:42</td>\n",
       "      <td>RT @brian_day15: I swear, I'm not sexist, but I honestly just cannot stand the woman college football announcer on ESPN2</td>\n",
       "      <td>[{'screen_name': 'brian_day15', 'name': 'Brian Day', 'id': 392181216, 'id_str': '392181216', 'indices': [3, 15]}]</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Unnamed: 0         Authors  \\\n",
       "ID                                              \n",
       "319399851215433729  2004       CreatrixKali     \n",
       "320817818222358529  5148       quincepoacher    \n",
       "324114200450437120  5167       MarkTramby       \n",
       "326286656854454273  5221       BinnyD           \n",
       "381988216292655104  3612       YesYoureSexist   \n",
       "\n",
       "                                                                                                                                                                        Descriptions  \\\n",
       "ID                                                                                                                                                                                     \n",
       "319399851215433729  Literary Creatrix for Alternative SpiritMag. Composer of articles of quirk and sometimes even remotely interesting status updates. And I'm in a book! Link below   \n",
       "320817818222358529  AKA Queenotisblue. Sociology, politics, policy, food, Gen Xer, Coburger & grumpy old lady. Social justice, education & employment stuff.                           \n",
       "324114200450437120  Mad keen Manly Sea Eagles supporter, love red wine (the better stuff), cricket ( all forms) and my wonderful family                                                \n",
       "326286656854454273  Reclusive nillionaire Whovian who enjoys television, arts, crafts and poking dead things with a stick.                                                             \n",
       "381988216292655104  Inspired by @YesYoureRacist.                                                                                                                                       \n",
       "\n",
       "                   Favorite Count Follower Count  \\\n",
       "ID                                                 \n",
       "319399851215433729  0              169             \n",
       "320817818222358529  0              425             \n",
       "324114200450437120  0              330             \n",
       "326286656854454273  0              211             \n",
       "381988216292655104  0              24041           \n",
       "\n",
       "                                                                                                Hashtags  \\\n",
       "ID                                                                                                         \n",
       "319399851215433729  [{'text': 'MKR', 'indices': [37, 41]}]                                                 \n",
       "320817818222358529  [{'text': 'MKR', 'indices': [68, 72]}]                                                 \n",
       "324114200450437120  [{'text': 'MKR', 'indices': [16, 20]}, {'text': 'TheVoiceAu', 'indices': [92, 103]}]   \n",
       "326286656854454273  [{'text': 'MKR', 'indices': [18, 22]}]                                                 \n",
       "381988216292655104  []                                                                                     \n",
       "\n",
       "                         Locations         Time Tweeted  \\\n",
       "ID                                                        \n",
       "319399851215433729  Australia       2013-04-03 10:43:53   \n",
       "320817818222358529  Melbourne       2013-04-07 08:38:23   \n",
       "324114200450437120  Brisbane        2013-04-16 10:57:01   \n",
       "326286656854454273  NSW, Australia  2013-04-22 10:49:35   \n",
       "381988216292655104                  2013-09-23 03:47:42   \n",
       "\n",
       "                                                                                                                                      Tweets  \\\n",
       "ID                                                                                                                                             \n",
       "319399851215433729  Oh yeah Colin! Smash those girls! :D #MKR                                                                                  \n",
       "320817818222358529  It's insane they keep bringing people back. When will this show end #MKR                                                   \n",
       "324114200450437120  @berkeley_eagle #MKR  this shit show has more comebacks than Johnny Farnham, ok back to the #TheVoiceAu                    \n",
       "326286656854454273  *sigh* oh Colin ? #MKR                                                                                                     \n",
       "381988216292655104  RT @brian_day15: I swear, I'm not sexist, but I honestly just cannot stand the woman college football announcer on ESPN2   \n",
       "\n",
       "                                                                                                                        User Mentions  \\\n",
       "ID                                                                                                                                      \n",
       "319399851215433729  []                                                                                                                  \n",
       "320817818222358529  []                                                                                                                  \n",
       "324114200450437120  [{'screen_name': 'berkeley_eagle', 'name': 'john G', 'id': 33840083, 'id_str': '33840083', 'indices': [0, 15]}]     \n",
       "326286656854454273  []                                                                                                                  \n",
       "381988216292655104  [{'screen_name': 'brian_day15', 'name': 'Brian Day', 'id': 392181216, 'id_str': '392181216', 'indices': [3, 15]}]   \n",
       "\n",
       "                     Class  \n",
       "ID                          \n",
       "319399851215433729  sexism  \n",
       "320817818222358529  none    \n",
       "324114200450437120  none    \n",
       "326286656854454273  none    \n",
       "381988216292655104  sexism  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = df[['Tweets','Class']]\n",
    "# df = df.dropna(subset=['Label'])\n",
    "df_short = df_short.dropna()\n",
    "df_short.columns = ['data','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>319399851215433729</th>\n",
       "      <td>Oh yeah Colin! Smash those girls! :D #MKR</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320817818222358529</th>\n",
       "      <td>It's insane they keep bringing people back. When will this show end #MKR</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324114200450437120</th>\n",
       "      <td>@berkeley_eagle #MKR  this shit show has more comebacks than Johnny Farnham, ok back to the #TheVoiceAu</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326286656854454273</th>\n",
       "      <td>*sigh* oh Colin ? #MKR</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381988216292655104</th>\n",
       "      <td>RT @brian_day15: I swear, I'm not sexist, but I honestly just cannot stand the woman college football announcer on ESPN2</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                        data  \\\n",
       "ID                                                                                                                                             \n",
       "319399851215433729  Oh yeah Colin! Smash those girls! :D #MKR                                                                                  \n",
       "320817818222358529  It's insane they keep bringing people back. When will this show end #MKR                                                   \n",
       "324114200450437120  @berkeley_eagle #MKR  this shit show has more comebacks than Johnny Farnham, ok back to the #TheVoiceAu                    \n",
       "326286656854454273  *sigh* oh Colin ? #MKR                                                                                                     \n",
       "381988216292655104  RT @brian_day15: I swear, I'm not sexist, but I honestly just cannot stand the woman college football announcer on ESPN2   \n",
       "\n",
       "                     label  \n",
       "ID                          \n",
       "319399851215433729  sexism  \n",
       "320817818222358529  none    \n",
       "324114200450437120  none    \n",
       "326286656854454273  none    \n",
       "381988216292655104  sexism  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_links(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ', ')    \n",
    "    return text\n",
    "\n",
    "#code source is http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "\n",
    "import re\n",
    "cList = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"i'm\": \"i am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you will\",\n",
    "  \"you'll've\": \"you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expand_contractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    text = c_re.sub(replace, text.lower())\n",
    "    return text\n",
    "\n",
    "def strip_mentions_hashtags(text):\n",
    "    entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def high_precision(text):\n",
    "    for token in text:\n",
    "        words = text.split()\n",
    "        assert words.isLower() == word.isUpper()\n",
    "        \n",
    "        \n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    #text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_short\n",
    "\n",
    "# data cleaning and processing\n",
    "df['data'] = df['data'].apply(lambda x : strip_links(x))\n",
    "df['data'] = df['data'].apply(lambda x : expand_contractions(x))\n",
    "df['data'] = df['data'].apply(lambda x : strip_mentions_hashtags(x))\n",
    "df['data'] = df['data'].apply(lambda x : remove_special_characters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('hatespeech_preporcess.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting dataset into train and validation set (80% - 20% default)\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['data'], df['label'],)\n",
    "\n",
    "\n",
    "#Label Encoder converts yes and no into 1 and 0\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "#Count vectorizer will calculate count of every word in text data and will ignore number and whitespaces\n",
    "\n",
    "count_vector = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vector.fit(df['data'])\n",
    "\n",
    "#using the count vector defined above, we'll transform our existing text data into train_x_count where every row will indicate \n",
    "#tweet and every column will represent count of word indexed at that loc\n",
    "\n",
    "train_x_count = count_vector.transform(train_x)\n",
    "valid_x_count = count_vector.transform(valid_x)\n",
    "\n",
    "#word level tf-idf\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(df['data'])\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_x)\n",
    "\n",
    "\n",
    "#ngram level tf-idf\n",
    "tfidf_ngram = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}', ngram_range=(2,3) ,max_features=5000)\n",
    "tfidf_ngram.fit(df['data'])\n",
    "xtrain_tfidf_ngram = tfidf_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram = tfidf_ngram.transform(valid_x)\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(df['data'])\n",
    "word_index = token.word_index\n",
    "\n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "none      10624\n",
       "sexism    3103 \n",
       "racism    1921 \n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words=None, max_features=10000, ngram_range=(1, 3))\n",
    "lr = LogisticRegression()\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def lr_cv(splits, X, Y, pipeline, average_method):\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=777)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    for train, test in kfold.split(X, Y):\n",
    "        lr_fit = pipeline.fit(X[train], Y[train])\n",
    "        prediction = lr_fit.predict(X[test])\n",
    "        scores = lr_fit.score(X[test],Y[test])\n",
    "        \n",
    "        accuracy.append(scores * 100)\n",
    "        precision.append(precision_score(Y[test], prediction, average=average_method)*100)\n",
    "        print('              None    Sexism     Racism')\n",
    "        print('precision:',precision_score(Y[test], prediction, average=None))\n",
    "        recall.append(recall_score(Y[test], prediction, average=average_method)*100)\n",
    "        print('recall:   ',recall_score(Y[test], prediction, average=None))\n",
    "        f1.append(f1_score(Y[test], prediction, average=average_method)*100)\n",
    "        print('f1 score: ',f1_score(Y[test], prediction, average=None))\n",
    "        print('-'*50)\n",
    "\n",
    "    print(\"accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(accuracy), np.std(accuracy)))\n",
    "    print(\"precision: %.2f%% (+/- %.2f%%)\" % (np.mean(precision), np.std(precision)))\n",
    "    print(\"recall: %.2f%% (+/- %.2f%%)\" % (np.mean(recall), np.std(recall)))\n",
    "    print(\"f1 score: %.2f%% (+/- %.2f%%)\" % (np.mean(f1), np.std(f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              None    Sexism     Racism\n",
      "('precision:', array([0.80196464, 0.82539683, 0.86826347]))\n",
      "('recall:   ', array([0.96047059, 0.54025974, 0.46698873]))\n",
      "('f1 score: ', array([0.87408994, 0.65306122, 0.60732984]))\n",
      "--------------------------------------------------\n",
      "              None    Sexism     Racism\n",
      "('precision:', array([0.79290172, 0.8313253 , 0.84227129]))\n",
      "('recall:   ', array([0.95670588, 0.5390625 , 0.42995169]))\n",
      "('f1 score: ', array([0.86713585, 0.65402844, 0.56929638]))\n",
      "--------------------------------------------------\n",
      "              None    Sexism     Racism\n",
      "('precision:', array([0.79441427, 0.83739837, 0.86928105]))\n",
      "('recall:   ', array([0.96376471, 0.53645833, 0.42834138]))\n",
      "('f1 score: ', array([0.87093345, 0.65396825, 0.57389428]))\n",
      "--------------------------------------------------\n",
      "              None    Sexism     Racism\n",
      "('precision:', array([0.791423  , 0.8372093 , 0.82026144]))\n",
      "('recall:   ', array([0.95529412, 0.5625    , 0.40483871]))\n",
      "('f1 score: ', array([0.86567164, 0.6728972 , 0.54211663]))\n",
      "--------------------------------------------------\n",
      "              None    Sexism     Racism\n",
      "('precision:', array([0.79052467, 0.78515625, 0.82075472]))\n",
      "('recall:   ', array([0.95056497, 0.5234375 , 0.42096774]))\n",
      "('f1 score: ', array([0.8631894, 0.628125 , 0.5565032]))\n",
      "--------------------------------------------------\n",
      "accuracy: 80.16% (+/- 0.61%)\n",
      "precision: 82.06% (+/- 1.26%)\n",
      "recall: 64.26% (+/- 0.78%)\n",
      "f1 score: 69.68% (+/- 0.93%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "original_pipeline = Pipeline([\n",
    "    ('vectorizer', tvec),\n",
    "    ('classifier', lr)\n",
    "])\n",
    "lr_cv(5, df.data, df.label, original_pipeline, 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "ROS_pipeline = make_pipeline(tvec, RandomOverSampler(random_state=777),lr)\n",
    "SMOTE_pipeline = make_pipeline(tvec, SMOTE(random_state=777),lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
      "Collecting imblearn\n",
      "  Downloading https://files.pythonhosted.org/packages/81/a7/4179e6ebfd654bd0eac0b9c06125b8b4c96a9d0a8ff9e9507eb2a26d2d7e/imblearn-0.0-py2.py3-none-any.whl\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/4d/e260fc004307d6ebc4909ee25e6c918a2399a7fb91975afd95ec48d1c8b4/imbalanced-learn-0.4.3.tar.gz (169kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 3.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages (from imbalanced-learn->imblearn) (1.16.4)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages (from imbalanced-learn->imblearn) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages (from imbalanced-learn->imblearn) (0.20.3)\n",
      "Building wheels for collected packages: imbalanced-learn\n",
      "  Building wheel for imbalanced-learn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/i504969/Library/Caches/pip/wheels/94/6c/0c/d7254937a767ff72814aa542997d0e889bed37c1d31ba3de1a\n",
      "Successfully built imbalanced-learn\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.4.3 imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              None    Sexism     Racism\n",
      "('precision:', array([0.87350959, 0.69396552, 0.5799458 ]))\n",
      "('recall:   ', array([0.79294118, 0.83636364, 0.68921095]))\n",
      "('f1 score: ', array([0.83127775, 0.75853946, 0.62987491]))\n",
      "--------------------------------------------------\n",
      "              None    Sexism     Racism\n",
      "('precision:', array([0.85075403, 0.64937759, 0.54068966]))\n",
      "('recall:   ', array([0.76988235, 0.81510417, 0.63123994]))\n",
      "('f1 score: ', array([0.8083004 , 0.72286374, 0.58246657]))\n",
      "--------------------------------------------------\n",
      "              None    Sexism     Racism\n",
      "('precision:', array([0.86252546, 0.68736142, 0.57202797]))\n",
      "('recall:   ', array([0.79717647, 0.80729167, 0.65861514]))\n",
      "('f1 score: ', array([0.82856444, 0.74251497, 0.61227545]))\n",
      "--------------------------------------------------\n",
      "              None    Sexism     Racism\n",
      "('precision:', array([0.86208678, 0.69164882, 0.56060606]))\n",
      "('recall:   ', array([0.78541176, 0.84114583, 0.65645161]))\n",
      "('f1 score: ', array([0.82196503, 0.75910693, 0.60475483]))\n",
      "--------------------------------------------------\n",
      "              None    Sexism     Racism\n",
      "('precision:', array([0.86607143, 0.63508065, 0.56730769]))\n",
      "('recall:   ', array([0.77636535, 0.8203125 , 0.66612903]))\n",
      "('f1 score: ', array([0.81876862, 0.71590909, 0.61275964]))\n",
      "--------------------------------------------------\n",
      "accuracy: 76.46% (+/- 1.02%)\n",
      "precision: 69.95% (+/- 1.28%)\n",
      "recall: 75.62% (+/- 1.11%)\n",
      "f1 score: 72.33% (+/- 1.21%)\n"
     ]
    }
   ],
   "source": [
    "lr_cv(5, df.data, df.label, SMOTE_pipeline, 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              None    Sexism     Racism\n",
      "('precision:', array([0.89252078, 0.5877193 , 0.61243386]))\n",
      "('recall:   ', array([0.75811765, 0.87012987, 0.74557166]))\n",
      "('f1 score: ', array([0.81984733, 0.70157068, 0.6724764 ]))\n",
      "--------------------------------------------------\n",
      "              None    Sexism     Racism\n",
      "('precision:', array([0.8819209 , 0.56065574, 0.568     ]))\n",
      "('recall:   ', array([0.73458824, 0.890625  , 0.68599034]))\n",
      "('f1 score: ', array([0.80154044, 0.68812877, 0.6214442 ]))\n",
      "--------------------------------------------------\n",
      "              None    Sexism     Racism\n",
      "('precision:', array([0.88292683, 0.58185053, 0.60580913]))\n",
      "('recall:   ', array([0.76658824, 0.8515625 , 0.70531401]))\n",
      "('f1 score: ', array([0.82065491, 0.69133192, 0.65178571]))\n",
      "--------------------------------------------------\n",
      "              None    Sexism     Racism\n",
      "('precision:', array([0.87111362, 0.56198347, 0.55761589]))\n",
      "('recall:   ', array([0.72517647, 0.88541667, 0.67903226]))\n",
      "('f1 score: ', array([0.79147406, 0.6875632 , 0.61236364]))\n",
      "--------------------------------------------------\n",
      "              None    Sexism     Racism\n",
      "('precision:', array([0.87909605, 0.54697987, 0.57217848]))\n",
      "('recall:   ', array([0.73258004, 0.84895833, 0.70322581]))\n",
      "('f1 score: ', array([0.79917822, 0.66530612, 0.63096961]))\n",
      "--------------------------------------------------\n",
      "accuracy: 75.10% (+/- 1.35%)\n",
      "precision: 67.75% (+/- 1.37%)\n",
      "recall: 77.22% (+/- 1.06%)\n",
      "f1 score: 71.04% (+/- 1.35%)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "RUS_pipeline = make_pipeline(tvec, RandomUnderSampler(random_state=777),lr)\n",
    "NM1_pipeline = make_pipeline(tvec, NearMiss(ratio='not minority',random_state=777, version = 1),lr)\n",
    "NM2_pipeline = make_pipeline(tvec, NearMiss(ratio='not minority',random_state=777, version = 2),lr)\n",
    "# NM3_pipeline = make_pipeline(tvec, NearMiss(ratio=nm3_dict,random_state=777, version = 3, n_neighbors_ver3=4),lr)\n",
    "\n",
    "\n",
    "lr_cv(5, df.data, df.label, RUS_pipeline, 'macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting dataset into train and validation set (80% - 20% default)\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['data'], df['label'],)\n",
    "\n",
    "\n",
    "#Label Encoder converts yes and no into 1 and 0\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "#Count vectorizer will calculate count of every word in text data and will ignore number and whitespaces\n",
    "\n",
    "count_vector = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vector.fit(df['data'])\n",
    "\n",
    "#using the count vector defined above, we'll transform our existing text data into train_x_count where every row will indicate \n",
    "#tweet and every column will represent count of word indexed at that loc\n",
    "\n",
    "train_x_count = count_vector.transform(train_x)\n",
    "valid_x_count = count_vector.transform(valid_x)\n",
    "\n",
    "#word level tf-idf\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(df['data'])\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_x)\n",
    "\n",
    "\n",
    "#ngram level tf-idf\n",
    "tfidf_ngram = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}', ngram_range=(2,3) ,max_features=5000)\n",
    "tfidf_ngram.fit(df['data'])\n",
    "xtrain_tfidf_ngram = tfidf_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram = tfidf_ngram.transform(valid_x)\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(df['data'])\n",
    "word_index = token.word_index\n",
    "\n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting dataset into train and validation set (80% - 20% default)\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['data'], df['label'])\n",
    "\n",
    "\n",
    "#Label Encoder converts yes and no into 1 and 0\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "#Count vectorizer will calculate count of every word in text data and will ignore number and whitespaces\n",
    "\n",
    "count_vector = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vector.fit(df['data'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_count = count_vector.transform(train_x)\n",
    "valid_x_count = count_vector.transform(valid_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11736, 15460)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_count.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word level tf-idf\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(df['data'])\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11736x5000 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 147214 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>319399851215433729</th>\n",
       "      <td>oh yeah colin smash those girls d</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320817818222358529</th>\n",
       "      <td>it is insane they keep bringing people back when will this show end</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                   data  \\\n",
       "ID                                                                                        \n",
       "319399851215433729  oh yeah colin smash those girls d                                     \n",
       "320817818222358529  it is insane they keep bringing people back when will this show end   \n",
       "\n",
       "                     label  \n",
       "ID                          \n",
       "319399851215433729  sexism  \n",
       "320817818222358529  none    "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_very_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_count = count_vector.fit(df_very_short['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=count_vector.transform(train_x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'back',\n",
       " u'bringing',\n",
       " u'colin',\n",
       " u'd',\n",
       " u'end',\n",
       " u'girls',\n",
       " u'insane',\n",
       " u'is',\n",
       " u'it',\n",
       " u'keep',\n",
       " u'oh',\n",
       " u'people',\n",
       " u'show',\n",
       " u'smash',\n",
       " u'they',\n",
       " u'this',\n",
       " u'those',\n",
       " u'when',\n",
       " u'will',\n",
       " u'yeah']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11736, 20)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11736,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'unicode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-44ea2763fcba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ata = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False);\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlda_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaMulticore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages/gensim/models/ldamulticore.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mgamma_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         )\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages/gensim/models/ldamodel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'unicode'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "# ata = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False);\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(df['data'], num_topics=10, id2word=train_x, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(train_x_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vector.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 20\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'pink', u'putting', u'walking', u'alive', u'catwalk', u'easily',\n",
       "       u'ways', u'forcing', u'animal', u'expected', u'pants', u'burned',\n",
       "       u'aisha', u'values', u'apply', u'slut', u'sauce', u'streets',\n",
       "       u'satan', u'liars'], dtype='<U78')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sexist_tweets = df[df[\"label\"]=='sexism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>319399851215433729</th>\n",
       "      <td>oh yeah colin smash those girls d</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381988216292655104</th>\n",
       "      <td>rt day15 i swear i am not sexist but i honestly just cannot stand the woman college football announcer on espn2</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381989237072990208</th>\n",
       "      <td>rt call me sexist but i think some women are seriously lacking knowledge l when it comes to feminism</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381989256404533251</th>\n",
       "      <td>rt call me sexist but females really need to stop acting like they are big football fans just stop</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381989513691529216</th>\n",
       "      <td>rt the mine is no place for a woman to work call me sexist but a woman as a rock driller is not practical</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                               data  \\\n",
       "ID                                                                                                                                    \n",
       "319399851215433729  oh yeah colin smash those girls d                                                                                 \n",
       "381988216292655104  rt day15 i swear i am not sexist but i honestly just cannot stand the woman college football announcer on espn2   \n",
       "381989237072990208  rt call me sexist but i think some women are seriously lacking knowledge l when it comes to feminism              \n",
       "381989256404533251  rt call me sexist but females really need to stop acting like they are big football fans just stop                \n",
       "381989513691529216  rt the mine is no place for a woman to work call me sexist but a woman as a rock driller is not practical         \n",
       "\n",
       "                     label  \n",
       "ID                          \n",
       "319399851215433729  sexism  \n",
       "381988216292655104  sexism  \n",
       "381989237072990208  sexism  \n",
       "381989256404533251  sexism  \n",
       "381989513691529216  sexism  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sexist_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_sexist, valid_x_sexist, train_y_sexist, valid_y_sexist = model_selection.train_test_split(df_sexist_tweets['data'], df_sexist_tweets['label'])\n",
    "\n",
    "\n",
    "count_vector = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vector.fit(df_sexist_tweets['data'])\n",
    "\n",
    "#using the count vector defined above, we'll transform our existing text data into train_x_count where every row will indicate \n",
    "#tweet and every column will represent count of word indexed at that loc\n",
    "\n",
    "train_x_count_sexist = count_vector.transform(train_x_sexist)\n",
    "valid_x_count_sexist = count_vector.transform(valid_x_sexist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(train_x_count_sexist)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vector.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 20\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'says cunt history means lie attractive truth nothing ladies vagina pussy manu beautiful well six arm ahem sucking sign fail',\n",
       " u'kat bitch oh male even thing hope fucking everyone best god think last tell feminism models got like rt point',\n",
       " u'someone nice mom head give celine sister roads uh account cleaning across followers humor founder money top examples elephant synonym',\n",
       " u'promo yes joke run ash repeat tennis putting engage k shes found trolling enough evidently whos 93 students heads setting',\n",
       " u'one saw go trying tonight colin friend death becoming halim topical chrisdowns poor sudden please 33 meat board tight mistake',\n",
       " u'nikki katie people 3 rt child must put either years choice able equality thought alone rules 1 sick honestly 2',\n",
       " u'new day talking exactly tart full deconstructed u econ sausage second cock shit anyway weeks feed properly steve dishes hateful',\n",
       " u'ever year else explain hard deserve idea needs seen abuse gets get believe human first round competition wa better worst',\n",
       " u'rt women sexist men sports 9 fuck football call im much funny rights sorry drive equal talk drivers dumb hate',\n",
       " u'kat andre person would horrible another nasty scored get amp u tell fri monoxide filled fucked student eliminated put shit',\n",
       " u'female rt sexist hate pretty right blondes could need us made think comedians 9 please always hot 2 like anything',\n",
       " u'amp feminist rt actually thanks use think find annie feminism many life word cooking nope movement kat answer crazy 2',\n",
       " u'make know sense ass sassy slut rt respect notice bit tough common based fucking remember changed h according anymore classy',\n",
       " u'rt sexist like get look girls good im girl back females know wrong every time woman would mean great see',\n",
       " u'want face kat ok form go wait fan google see kick slap abortion list stupid score kats attractiveness hey set',\n",
       " u'two way said none well might http tweets enough front cabbies feminism people goal simple drive problem physically buy created',\n",
       " u'blonde wonder safer dead durrie bum figure dudes dishes compliment cooks gone ghosts shirt 23 together beaten satan ten agreed',\n",
       " u'rt sexist woman women call think feminists really would stand man watch take still lol football lot even cook know',\n",
       " u'never walk cares away standup close met tv silly missed huh ufc ghost annoyed 2016 aware tbh intentionally blank screen',\n",
       " u'girls better rt looking tweet annoying going yeah shave boys punch november super argue 50 live carbon sociopath sass mood']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sample sentence , showing stop words filtration .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def stop_word_removal(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in text_tokens if not w in stop_words] \n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "df_sexist_tweets['data'] = df_sexist_tweets['data'].apply(lambda x : stop_word_removal(x))\n",
    "  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_sexist_tweets['data'] = df_sexist_tweets['data'].apply(lambda x : stop_word_removal(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_racism_tweets = df[df[\"label\"]=='racism']\n",
    "\n",
    "train_x_racism, valid_x_racism, train_y_racism, valid_y_racism = model_selection.train_test_split(df_racism_tweets['data'], df_racism_tweets['label'])\n",
    "\n",
    "\n",
    "count_vector = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vector.fit(df_racism_tweets['data'])\n",
    "\n",
    "#using the count vector defined above, we'll transform our existing text data into train_x_count where every row will indicate \n",
    "#tweet and every column will represent count of word indexed at that loc\n",
    "\n",
    "train_x_count_racism = count_vector.transform(train_x_racism)\n",
    "valid_x_count_racsim = count_vector.transform(valid_x_racism)\n",
    "\n",
    "\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(train_x_count_racism)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vector.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 20\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'arab', u'liars', u'justice', u'self', u'occupied', u'serving',\n",
       "       u'genociadal', u'araujog', u'exterminated', u'to', u'hate', u'of',\n",
       "       u'minorities', u'people', u'that', u'every', u'democracy',\n",
       "       u'muslims', u'freedom', u'me'], dtype='<U40')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import csv\n",
    "\n",
    "#Twitter API credentials\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_key = \"\"\n",
    "access_secret = \"\"\n",
    "\n",
    "\n",
    "def get_all_tweets(screen_name):\n",
    "\t#Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "\t\n",
    "\t#authorize twitter, initialize tweepy\n",
    "\tauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "\tauth.set_access_token(access_key, access_secret)\n",
    "\tapi = tweepy.API(auth)\n",
    "\t\n",
    "\t#initialize a list to hold all the tweepy Tweets\n",
    "\talltweets = []\t\n",
    "\t\n",
    "\t#make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "\tnew_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
    "\t\n",
    "\t#save most recent tweets\n",
    "\talltweets.extend(new_tweets)\n",
    "\t\n",
    "\t#save the id of the oldest tweet less one\n",
    "\toldest = alltweets[-1].id - 1\n",
    "\t\n",
    "\t#keep grabbing tweets until there are no tweets left to grab\n",
    "\twhile len(new_tweets) > 0:\n",
    "\t\tprint \"getting tweets before %s\" % (oldest)\n",
    "\t\t\n",
    "\t\t#all subsiquent requests use the max_id param to prevent duplicates\n",
    "\t\tnew_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
    "\t\t\n",
    "\t\t#save most recent tweets\n",
    "\t\talltweets.extend(new_tweets)\n",
    "\t\t\n",
    "\t\t#update the id of the oldest tweet less one\n",
    "\t\toldest = alltweets[-1].id - 1\n",
    "\t\t\n",
    "\t\tprint \"...%s tweets downloaded so far\" % (len(alltweets))\n",
    "\t\n",
    "\t#transform the tweepy tweets into a 2D array that will populate the csv\t\n",
    "\touttweets = [[tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\")] for tweet in alltweets]\n",
    "\t\n",
    "\t#write the csv\t\n",
    "\twith open('%s_tweets.csv' % screen_name, 'wb') as f:\n",
    "\t\twriter = csv.writer(f)\n",
    "\t\twriter.writerow([\"id\",\"created_at\",\"text\"])\n",
    "\t\twriter.writerows(outtweets)\n",
    "\t\n",
    "\tpass\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t#pass in the username of the account you want to download\n",
    "\tget_all_tweets(\"J_tsar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "  \n",
    "stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "word_tokens = word_tokenize(example_sent) \n",
    "  \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "filtered_sentence = [] \n",
    "\n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import sys,json,math\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "def load_word2vec(filename):\n",
    "    # Returns a dict containing a {word: numpy array for a dense word vector} mapping.\n",
    "    # It loads everything into memory.\n",
    "    \n",
    "    w2vec={}\n",
    "    with open(filename,\"r\") as f_in:\n",
    "        for line in f_in:\n",
    "            line_split=line.replace(\"\\n\",\"\").split()\n",
    "            w=line_split[0]\n",
    "            vec=np.array([float(x) for x in line_split[1:]])\n",
    "            w2vec[w]=vec\n",
    "    return w2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emb_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-43ff99a35239>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memb_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'emb_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "('Done.', 1193514, ' words loaded!')\n"
     ]
    }
   ],
   "source": [
    "word_to_vec_dict = loadGloveModel('glove.twitter.27B.200d.txt')\n",
    "\n",
    "import numpy\n",
    "emb_matrix = numpy.zeros((len(word_index)+1, 200))\n",
    "for word, i in word_index.items():\n",
    "    emb_vect = word_to_vec_dict.get(word)\n",
    "    if emb_vect is not None:\n",
    "        emb_matrix[i] = emb_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.DataFrame(train_x)\n",
    "dataset_valid = pd.DataFrame(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_topic_model = pd.concat([dataset_train, dataset_valid], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dataset_for_topic_model['data'])\n",
    "lda = LatentDirichletAllocation(random_state=0)\n",
    "doc_topic_matrix = lda.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train['comp'] = dataset_train['data'].apply(lambda x : analyser.polarity_scores(x)['compound'])\n",
    "dataset_train['pos'] = dataset_train['data'].apply(lambda x : analyser.polarity_scores(x)['pos'])\n",
    "dataset_train['neu'] = dataset_train['data'].apply(lambda x : analyser.polarity_scores(x)['neu'])\n",
    "dataset_train['neg'] = dataset_train['data'].apply(lambda x : analyser.polarity_scores(x)['neg'])\n",
    "\n",
    "dataset_valid['comp'] = dataset_valid['data'].apply(lambda x : analyser.polarity_scores(x)['compound'])\n",
    "dataset_valid['pos'] = dataset_valid['data'].apply(lambda x : analyser.polarity_scores(x)['pos'])\n",
    "dataset_valid['neu'] = dataset_valid['data'].apply(lambda x : analyser.polarity_scores(x)['neu'])\n",
    "dataset_valid['neg'] = dataset_valid['data'].apply(lambda x : analyser.polarity_scores(x)['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine sentiment features with count vectorizer features\n",
    "train_x_count = pd.DataFrame(np.concatenate((train_x_count.toarray(), dataset_train[['comp', 'pos', 'neu', 'neg']].values, doc_topic_matrix[:len(train_x)]), axis=1))\n",
    "valid_x_count = pd.DataFrame(np.concatenate((valid_x_count.toarray(), dataset_valid[['comp', 'pos', 'neu', 'neg']].values, doc_topic_matrix[len(train_x):]), axis=1))\n",
    "\n",
    "# combine sentiment features with tf-idf features\n",
    "xtrain_tfidf = pd.DataFrame(np.concatenate((xtrain_tfidf.toarray(), dataset_train[['comp', 'pos', 'neu', 'neg']].values, doc_topic_matrix[:len(train_x)]), axis=1))\n",
    "xvalid_tfidf = pd.DataFrame(np.concatenate((xvalid_tfidf.toarray(), dataset_valid[['comp', 'pos', 'neu', 'neg']].values, doc_topic_matrix[len(train_x):]), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_model(classifier, feature_vector_train,label, feature_vector_valid, is_neural_net = False):\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    prediction = classifier.predict(feature_vector_valid)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    if not is_neural_net:\n",
    "        print(confusion_matrix(valid_y, prediction))\n",
    "    if is_neural_net:\n",
    "        prediction = prediction.argmax(axis=-1)\n",
    "    return metrics.accuracy_score(prediction, valid_y), f1_score(prediction, valid_y,average=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2307  169  153]\n",
      " [ 135  346    3]\n",
      " [ 345    4  450]]\n",
      "('Naive Bayes (Count Vectors)', 0.793200408997955, array([0.85192024, 0.68993021, 0.6405694 ]))\n",
      "[[2568   36   25]\n",
      " [ 294  190    0]\n",
      " [ 551    0  248]]\n",
      "('Naive Bayes (TDIDF)', 0.7684049079754601, array([0.85004965, 0.53521127, 0.46268657]))\n"
     ]
    }
   ],
   "source": [
    "count_vector = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vector.fit(df['data'])\n",
    "nb_train_x_count = count_vector.transform(train_x)\n",
    "nb_valid_x_count = count_vector.transform(valid_x)\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(df['data'])\n",
    "nb_xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "nb_xvalid_tfidf = tfidf_vect.transform(valid_x)\n",
    "\n",
    "\n",
    "\n",
    "clf = naive_bayes.MultinomialNB()\n",
    "accuracy, f1 = train_model(clf, nb_train_x_count, train_y, nb_valid_x_count)\n",
    "print(\"Naive Bayes (Count Vectors)\",accuracy, f1)\n",
    "accuracy, f1 = train_model(clf, nb_xtrain_tfidf, train_y, nb_xvalid_tfidf)\n",
    "print(\"Naive Bayes (TDIDF)\", accuracy, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][[2434   86  113]\n",
      " [ 149  334    4]\n",
      " [ 318    2  472]]\n",
      "('LR using CV', 0.8282208588957055, array([0.87965305, 0.73487349, 0.68356264]))\n",
      "[LibLinear][[2495   68   70]\n",
      " [ 207  275    5]\n",
      " [ 417    0  375]]\n",
      "('LR using CV', 0.8039366053169734, array([0.86752434, 0.6626506 , 0.60386473]))\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.LogisticRegression(verbose=1)\n",
    "accuracy, f1 = train_model(clf, train_x_count, train_y, valid_x_count)\n",
    "print(\"LR using CV\", accuracy, f1)\n",
    "accuracy, f1 = train_model(clf, xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"LR using CV\", accuracy, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1 = train_model(xgboost.XGBClassifier(verbose=1), train_x_count, train_y, valid_x_count)\n",
    "print(\"xgboost using CV\", accuracy, f1)\n",
    "accuracy, f1 = train_model(xgboost.XGBClassifier(verbose=1), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"xgboost using TFIDF\", accuracy, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shallow Neural Networks\n",
    "\n",
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11736/11736 [==============================] - 5s 419us/step - loss: 0.4249\n",
      "Epoch 2/20\n",
      "11736/11736 [==============================] - 4s 361us/step - loss: -0.4221\n",
      "Epoch 3/20\n",
      "11736/11736 [==============================] - 4s 378us/step - loss: -0.7532\n",
      "Epoch 4/20\n",
      "11736/11736 [==============================] - 4s 343us/step - loss: -0.9797\n",
      "Epoch 5/20\n",
      "11736/11736 [==============================] - 4s 339us/step - loss: -1.1788\n",
      "Epoch 6/20\n",
      "11736/11736 [==============================] - 4s 336us/step - loss: -1.3251\n",
      "Epoch 7/20\n",
      "11736/11736 [==============================] - 4s 338us/step - loss: -1.4424\n",
      "Epoch 8/20\n",
      "11736/11736 [==============================] - 4s 339us/step - loss: -1.5396\n",
      "Epoch 9/20\n",
      "11736/11736 [==============================] - 4s 338us/step - loss: -1.6192\n",
      "Epoch 10/20\n",
      "11736/11736 [==============================] - 4s 337us/step - loss: -1.6858\n",
      "Epoch 11/20\n",
      "11736/11736 [==============================] - 4s 337us/step - loss: -1.7446\n",
      "Epoch 12/20\n",
      "11736/11736 [==============================] - 4s 336us/step - loss: -1.7928\n",
      "Epoch 13/20\n",
      "11736/11736 [==============================] - 4s 339us/step - loss: -1.8363\n",
      "Epoch 14/20\n",
      "11736/11736 [==============================] - 4s 338us/step - loss: -1.8740\n",
      "Epoch 15/20\n",
      "11736/11736 [==============================] - 4s 338us/step - loss: -1.9151\n",
      "Epoch 16/20\n",
      "11736/11736 [==============================] - 4s 337us/step - loss: -1.9417\n",
      "Epoch 17/20\n",
      "11736/11736 [==============================] - 4s 336us/step - loss: -1.9786\n",
      "Epoch 18/20\n",
      "11736/11736 [==============================] - 4s 335us/step - loss: -2.0036\n",
      "Epoch 19/20\n",
      "11736/11736 [==============================] - 4s 345us/step - loss: -2.0292\n",
      "Epoch 20/20\n",
      "11736/11736 [==============================] - 4s 345us/step - loss: -2.0578\n",
      "('NN, Ngram Level TF IDF Vectors', 0.6730572597137015)\n"
     ]
    }
   ],
   "source": [
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "accuracy, f1 = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "print (\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "11736/11736 [==============================] - 5s 415us/step - loss: 0.4216\n",
      "Epoch 2/2\n",
      "11736/11736 [==============================] - 4s 339us/step - loss: -0.4022\n",
      "('NN, Ngram Level TF IDF Vectors', 0.6730572597137015)\n"
     ]
    }
   ],
   "source": [
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "accuracy, f1 = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "print (\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Epoch 1/2\n",
      "11736/11736 [==============================] - 56s 5ms/step - loss: 0.0461\n",
      "Epoch 2/2\n",
      "11736/11736 [==============================] - 54s 5ms/step - loss: -0.3812\n",
      "(0.6730572597137015, array([0.80458365, 0.        , 0.        ]))\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_gru():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 200, weights=[emb_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the GRU Layer\n",
    "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_gru()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Add layers to model\n",
    "model.add(Dense(100, activation='relu', input_shape = (70,)))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_28 to have shape (3,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-5fba8e197ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#train_seq_x, train_y, valid_seq_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages/keras/engine/training_utils.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_28 to have shape (3,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(patience=2) \n",
    "#train_seq_x, train_y, valid_seq_x\n",
    "#fit model\n",
    "model.fit(train_seq_x, train_y, validation_split=0.3, epochs=20, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11736,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprop:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.document = 'document'\n",
    "    \n",
    "    def remove_strip_links(self,text):\n",
    "        link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "        links         = re.findall(link_regex, text)\n",
    "        for link in links:\n",
    "            text = text.replace(link[0], ', ')    \n",
    "        return text\n",
    "    \n",
    "    def strip_mentions_hashtags(self, text):\n",
    "        entity_prefixes = ['@','#']\n",
    "        for separator in  string.punctuation:\n",
    "            if separator not in entity_prefixes :\n",
    "                text = text.replace(separator,' ')\n",
    "        words = []\n",
    "        for word in text.split():\n",
    "            word = word.strip()\n",
    "            if word:\n",
    "                if word[0] not in entity_prefixes:\n",
    "                    words.append(word)\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def remove_special_characters(text, remove_digits=False):\n",
    "        pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "        text = re.sub(pattern, '', text)\n",
    "        #text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        return text\n",
    "        \n",
    "    def remove_non_ascii(self, words):\n",
    "        \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words.split():\n",
    "            word = word.strip()\n",
    "            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_words.append(new_word)\n",
    "        return ' '.join(new_words)\n",
    "\n",
    "    def to_lowercase(self, words):\n",
    "        \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words.split():\n",
    "            word = word.strip()\n",
    "            new_word = word.lower()\n",
    "            new_words.append(new_word)\n",
    "        return ' '.join(new_words)\n",
    "\n",
    "    def remove_punctuation(self, words):\n",
    "        \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words.split():\n",
    "            word = word.strip()\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "        return ' '.join(new_words)\n",
    "\n",
    "    def replace_numbers(self, words):\n",
    "        \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "        p = inflect.engine()\n",
    "        new_words = []\n",
    "        for word in words.split():\n",
    "            word = word.strip()\n",
    "            if word.isdigit():\n",
    "                new_word = p.number_to_words(word)\n",
    "                new_words.append(new_word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        return ' '.join(new_words)\n",
    "\n",
    "    def remove_stopwords(self,words):\n",
    "        \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words.split():\n",
    "            word = word.strip()\n",
    "            if word not in stopwords.words('english'):\n",
    "                new_words.append(word)\n",
    "        return ' '.join(new_words)\n",
    "\n",
    "    def stem_words(self, words):\n",
    "        \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "        stemmer = LancasterStemmer()\n",
    "        stems = []\n",
    "        for word in words.split():\n",
    "            word = word.strip()\n",
    "            stem = stemmer.stem(word)\n",
    "            stems.append(stem)\n",
    "        return ' '.join(stems)\n",
    "\n",
    "    def lemmatize_verbs(self, words):\n",
    "        \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmas = []\n",
    "        for word in words.split():\n",
    "            word = word.strip()\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "            lemmas.append(lemma)\n",
    "        return ' '.join(lemmas)\n",
    "\n",
    "    def normalize(self, words):\n",
    "        words = self.remove_strip_links(words)\n",
    "        words = self.strip_mentions_hashtags(words)\n",
    "        words = self.remove_non_ascii(words)\n",
    "        words = self.to_lowercase(words)\n",
    "        words = self.remove_punctuation(words)\n",
    "        words = self.replace_numbers(words)\n",
    "        words = self.remove_stopwords(words)\n",
    "        return words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "import string\n",
    "import unicodedata\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = Preprop()\n",
    "df_new['data'] = df_new['data'].apply(lambda x: instance.normalize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "319399851215433729    oh yeah colin smash girls                                                                                        \n",
       "320817818222358529    insane keep bringing people back show end                                                                        \n",
       "324114200450437120    eagle shit show comebacks johnny farnham ok back                                                                 \n",
       "326286656854454273    sigh oh colin                                                                                                    \n",
       "381988216292655104    rt day15 swear sexist honestly cannot stand woman college football announcer espn2                               \n",
       "381989167564996609    science good says men cannot monogamous shit amirite                                                             \n",
       "381989237072990208    rt call sexist think women seriously lacking knowledge l comes feminism                                          \n",
       "381989256404533251    rt call sexist females really need stop acting like big football fans stop                                       \n",
       "381989513691529216    rt mine place woman work call sexist woman rock driller practical                                                \n",
       "382350557957984256    rt naija call sexist last tweet women want equal praise success gotta take equal blame failur                    \n",
       "382350622411874305    rt sorry see hot girls eat like guys call sexist like girls girls                                                \n",
       "382351113204154368    every day think maybe day find sexist tweets never day keep good fight women                                     \n",
       "382518137582661632    rt honestly stereotypes light hearted usually harmless let people joke around                                    \n",
       "382518148617863168    rt feminism respectable ideal sensitive searching problems                                                       \n",
       "382518158470311936    rt would mad fact appreciate called bigot saying                                                                 \n",
       "382518676865294336    rt sexist tweet women sexism work ways                                                                           \n",
       "382518915454087168    sexism work ways stop today got misogynist bingo nine                                                            \n",
       "382606225520615424    rt look see tweets pure hatred look see light stupidity                                                          \n",
       "382606809715834880    educate oppression                                                                                               \n",
       "382868890306363392    read phrase feminist agenda snorted cereal yes let international conspiracy unveiled                             \n",
       "383627629766508545    rt would rather blind dog ref freakin liz sorry sexist girls                                                     \n",
       "383627770506399744    rt sexist women play football past age twelve unless two hundred eighty five                                     \n",
       "383627997950910464    true women twelve play football repeated concussions might make us stupid                                        \n",
       "383698416171118593    rt well followyou stop trolling tohave something better look sexist tw                                           \n",
       "383698818698457089    oh plenty picking rotten low hanging sexist fruit take much day                                                  \n",
       "383699280130617345    rt abel sexist saying men belong kitchen utterly true tried making instant oa                                    \n",
       "383730558863290368    rt okay fuck shit pussy                                                                                          \n",
       "383730982513168384    sexist litmus test call lvl two sexist comment amp dood comes back lvl ten guns ablazing pussy amp cunt amp bitch\n",
       "383731135668162560    femininity horrible sexist dude cries girl wedding dress vagina                                                  \n",
       "384042596680474624    rt kell leave washing mum complete remtard                                                                       \n",
       "                                         ...                                                                                           \n",
       "576760519777894400    palestinians train children hate israelis us confrontations propaganda                                           \n",
       "576761020498124800    lol angry men writing essays facebook messenger amount nicer harassed                                            \n",
       "576761231819718656    least assuming read two paragraphs got bored deleted message know guy                                            \n",
       "576761315756101632    joint blacks muslims murdering police ferguson                                                                   \n",
       "576764046411632640    ahh christian conspiracy book involving real christians actions unnoticed irrelevant                             \n",
       "576764285314957312    know hundreds christians single one fits discription                                                             \n",
       "576765290299568128    thousands churches temples synagogs destroyed muslim biggots worry one muslim                                    \n",
       "576765492448268288    mosque still standing typical muslim terrorist asshole                                                           \n",
       "576767285387354113    max looks genocide seven thousand ezidis countless christians muslims friends writes word                        \n",
       "576768402091446272    blame twenty four seven schedule promoting anti semitism                                                         \n",
       "576770361275990016    rt state dept shut part email network believed infected russian hackers                                          \n",
       "576770673550323712    putin literally thousands hired trolls working internet shape opinion putin propaganda                           \n",
       "576771329975664640    rt dear state department hillary clintons one hundred nine form                                                  \n",
       "576772044362158080    castro safar filthy islam created funding                                                                        \n",
       "576772176457547778    castro safar                                                                                                     \n",
       "576773965529550848    let us forget epstein mold american left great friends clintons                                                  \n",
       "576777529471569920    max palestinian muslim friends throw atheist blogger prison ten months religious views                           \n",
       "576778079386791936    muslims practically exterminated every non muslim minority mus countries                                         \n",
       "576778967320911872    castro safar goat calling terrorist horny                                                                        \n",
       "576780193047584768    nothing incorrect generalization look minority demographics                                                      \n",
       "576780412564848640    majority muslim country                                                                                          \n",
       "576780636930777088    nothing wrong stoping muslim genocide non muslims                                                                \n",
       "576781307612651521    castro safar kkk almost non existent muslims terrorists                                                          \n",
       "576781790334988288    minority india microbrain next door pakistan muslim mobs murdering                                               \n",
       "576781938339356672    hindus christians blasphemy                                                                                      \n",
       "588296672226058240    oh fuck hard rusty chainsaw another round instant restaurants                                                    \n",
       "590818760337924096    omg shut drasko bianca                                                                                           \n",
       "595166563516329985    ash found inner bogun ewwww                                                                                      \n",
       "618367060117098498    sorry real cunt                                                                                                  \n",
       "684779048120258560    chloe kelly seriously hate girls arrogant                                                                        \n",
       "Name: data, Length: 15648, dtype: object"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_links(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ', ')    \n",
    "    return text\n",
    "\n",
    "#code source is http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "\n",
    "import re\n",
    "cList = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"i'm\": \"i am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you will\",\n",
    "  \"you'll've\": \"you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expand_contractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    text = c_re.sub(replace, text.lower())\n",
    "    return text\n",
    "\n",
    "def strip_mentions_hashtags(text):\n",
    "    entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def high_precision(text):\n",
    "    for token in text:\n",
    "        words = text.split()\n",
    "        assert words.isLower() == word.isUpper()\n",
    "        \n",
    "        \n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    #text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 370943710856355839\n",
      "...46 tweets downloaded so far\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purvak hi how are you\n"
     ]
    }
   ],
   "source": [
    "x = 'purvak hi how are you'\n",
    "x.split()\n",
    "print x.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
