{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pprint\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import string\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "import nltk\n",
    "# from nltk.tokenize.t import ToktokTokenizer\n",
    "# tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "from gensim import corpora, models\n",
    "from keras import layers, models, optimizers\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "#preprocessing pipeline\n",
    "#Pipeline models features like word count, tfidf, word density, word embeddings (GloVe)\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from keras.preprocessing import text, sequence\n",
    "import xgboost\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = pd.read_csv('hatespeech.csv', encoding=\"ISO-8859-1\",index_col=6, keep_default_na=False)\n",
    "#print(hs.head())\n",
    "\n",
    "orig = pd.read_csv('NAACL_SRW_2016.csv', index_col=0, header=None)\n",
    "orig.index.name = 'ID'\n",
    "orig = orig.rename(columns={1: 'Class'})\n",
    "orig.index = orig.index.astype(str)\n",
    "#print(orig.head())\n",
    "\n",
    "#merging the two dataframes\n",
    "hs = pd.merge(hs, orig, how='inner', left_index=True, right_index=True)\n",
    "#print(hs.head())\n",
    "df = hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Descriptions</th>\n",
       "      <th>Favorite Count</th>\n",
       "      <th>Follower Count</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Locations</th>\n",
       "      <th>Time Tweeted</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>User Mentions</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>319399851215433729</th>\n",
       "      <td>2004</td>\n",
       "      <td>CreatrixKali</td>\n",
       "      <td>Literary Creatrix for Alternative SpiritMag. Composer of articles of quirk and sometimes even remotely interesting status updates. And I'm in a book! Link below</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "      <td>[{'text': 'MKR', 'indices': [37, 41]}]</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2013-04-03 10:43:53</td>\n",
       "      <td>Oh yeah Colin! Smash those girls! :D #MKR</td>\n",
       "      <td>[]</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320817818222358529</th>\n",
       "      <td>5148</td>\n",
       "      <td>quincepoacher</td>\n",
       "      <td>AKA Queenotisblue. Sociology, politics, policy, food, Gen Xer, Coburger &amp; grumpy old lady. Social justice, education &amp; employment stuff.</td>\n",
       "      <td>0</td>\n",
       "      <td>425</td>\n",
       "      <td>[{'text': 'MKR', 'indices': [68, 72]}]</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>2013-04-07 08:38:23</td>\n",
       "      <td>It's insane they keep bringing people back. When will this show end #MKR</td>\n",
       "      <td>[]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324114200450437120</th>\n",
       "      <td>5167</td>\n",
       "      <td>MarkTramby</td>\n",
       "      <td>Mad keen Manly Sea Eagles supporter, love red wine (the better stuff), cricket ( all forms) and my wonderful family</td>\n",
       "      <td>0</td>\n",
       "      <td>330</td>\n",
       "      <td>[{'text': 'MKR', 'indices': [16, 20]}, {'text': 'TheVoiceAu', 'indices': [92, 103]}]</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>2013-04-16 10:57:01</td>\n",
       "      <td>@berkeley_eagle #MKR  this shit show has more comebacks than Johnny Farnham, ok back to the #TheVoiceAu</td>\n",
       "      <td>[{'screen_name': 'berkeley_eagle', 'name': 'john G', 'id': 33840083, 'id_str': '33840083', 'indices': [0, 15]}]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326286656854454273</th>\n",
       "      <td>5221</td>\n",
       "      <td>BinnyD</td>\n",
       "      <td>Reclusive nillionaire Whovian who enjoys television, arts, crafts and poking dead things with a stick.</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>[{'text': 'MKR', 'indices': [18, 22]}]</td>\n",
       "      <td>NSW, Australia</td>\n",
       "      <td>2013-04-22 10:49:35</td>\n",
       "      <td>*sigh* oh Colin ? #MKR</td>\n",
       "      <td>[]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381988216292655104</th>\n",
       "      <td>3612</td>\n",
       "      <td>YesYoureSexist</td>\n",
       "      <td>Inspired by @YesYoureRacist.</td>\n",
       "      <td>0</td>\n",
       "      <td>24041</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>2013-09-23 03:47:42</td>\n",
       "      <td>RT @brian_day15: I swear, I'm not sexist, but I honestly just cannot stand the woman college football announcer on ESPN2</td>\n",
       "      <td>[{'screen_name': 'brian_day15', 'name': 'Brian Day', 'id': 392181216, 'id_str': '392181216', 'indices': [3, 15]}]</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Unnamed: 0         Authors  \\\n",
       "ID                                              \n",
       "319399851215433729  2004       CreatrixKali     \n",
       "320817818222358529  5148       quincepoacher    \n",
       "324114200450437120  5167       MarkTramby       \n",
       "326286656854454273  5221       BinnyD           \n",
       "381988216292655104  3612       YesYoureSexist   \n",
       "\n",
       "                                                                                                                                                                        Descriptions  \\\n",
       "ID                                                                                                                                                                                     \n",
       "319399851215433729  Literary Creatrix for Alternative SpiritMag. Composer of articles of quirk and sometimes even remotely interesting status updates. And I'm in a book! Link below   \n",
       "320817818222358529  AKA Queenotisblue. Sociology, politics, policy, food, Gen Xer, Coburger & grumpy old lady. Social justice, education & employment stuff.                           \n",
       "324114200450437120  Mad keen Manly Sea Eagles supporter, love red wine (the better stuff), cricket ( all forms) and my wonderful family                                                \n",
       "326286656854454273  Reclusive nillionaire Whovian who enjoys television, arts, crafts and poking dead things with a stick.                                                             \n",
       "381988216292655104  Inspired by @YesYoureRacist.                                                                                                                                       \n",
       "\n",
       "                   Favorite Count Follower Count  \\\n",
       "ID                                                 \n",
       "319399851215433729  0              169             \n",
       "320817818222358529  0              425             \n",
       "324114200450437120  0              330             \n",
       "326286656854454273  0              211             \n",
       "381988216292655104  0              24041           \n",
       "\n",
       "                                                                                                Hashtags  \\\n",
       "ID                                                                                                         \n",
       "319399851215433729  [{'text': 'MKR', 'indices': [37, 41]}]                                                 \n",
       "320817818222358529  [{'text': 'MKR', 'indices': [68, 72]}]                                                 \n",
       "324114200450437120  [{'text': 'MKR', 'indices': [16, 20]}, {'text': 'TheVoiceAu', 'indices': [92, 103]}]   \n",
       "326286656854454273  [{'text': 'MKR', 'indices': [18, 22]}]                                                 \n",
       "381988216292655104  []                                                                                     \n",
       "\n",
       "                         Locations         Time Tweeted  \\\n",
       "ID                                                        \n",
       "319399851215433729  Australia       2013-04-03 10:43:53   \n",
       "320817818222358529  Melbourne       2013-04-07 08:38:23   \n",
       "324114200450437120  Brisbane        2013-04-16 10:57:01   \n",
       "326286656854454273  NSW, Australia  2013-04-22 10:49:35   \n",
       "381988216292655104                  2013-09-23 03:47:42   \n",
       "\n",
       "                                                                                                                                      Tweets  \\\n",
       "ID                                                                                                                                             \n",
       "319399851215433729  Oh yeah Colin! Smash those girls! :D #MKR                                                                                  \n",
       "320817818222358529  It's insane they keep bringing people back. When will this show end #MKR                                                   \n",
       "324114200450437120  @berkeley_eagle #MKR  this shit show has more comebacks than Johnny Farnham, ok back to the #TheVoiceAu                    \n",
       "326286656854454273  *sigh* oh Colin ? #MKR                                                                                                     \n",
       "381988216292655104  RT @brian_day15: I swear, I'm not sexist, but I honestly just cannot stand the woman college football announcer on ESPN2   \n",
       "\n",
       "                                                                                                                        User Mentions  \\\n",
       "ID                                                                                                                                      \n",
       "319399851215433729  []                                                                                                                  \n",
       "320817818222358529  []                                                                                                                  \n",
       "324114200450437120  [{'screen_name': 'berkeley_eagle', 'name': 'john G', 'id': 33840083, 'id_str': '33840083', 'indices': [0, 15]}]     \n",
       "326286656854454273  []                                                                                                                  \n",
       "381988216292655104  [{'screen_name': 'brian_day15', 'name': 'Brian Day', 'id': 392181216, 'id_str': '392181216', 'indices': [3, 15]}]   \n",
       "\n",
       "                     Class  \n",
       "ID                          \n",
       "319399851215433729  sexism  \n",
       "320817818222358529  none    \n",
       "324114200450437120  none    \n",
       "326286656854454273  none    \n",
       "381988216292655104  sexism  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = df[['Tweets','Class']]\n",
    "# df = df.dropna(subset=['Label'])\n",
    "df_short = df_short.dropna()\n",
    "df_short.columns = ['data','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>319399851215433729</th>\n",
       "      <td>Oh yeah Colin! Smash those girls! :D #MKR</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320817818222358529</th>\n",
       "      <td>It's insane they keep bringing people back. When will this show end #MKR</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324114200450437120</th>\n",
       "      <td>@berkeley_eagle #MKR  this shit show has more comebacks than Johnny Farnham, ok back to the #TheVoiceAu</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326286656854454273</th>\n",
       "      <td>*sigh* oh Colin ? #MKR</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381988216292655104</th>\n",
       "      <td>RT @brian_day15: I swear, I'm not sexist, but I honestly just cannot stand the woman college football announcer on ESPN2</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                        data  \\\n",
       "ID                                                                                                                                             \n",
       "319399851215433729  Oh yeah Colin! Smash those girls! :D #MKR                                                                                  \n",
       "320817818222358529  It's insane they keep bringing people back. When will this show end #MKR                                                   \n",
       "324114200450437120  @berkeley_eagle #MKR  this shit show has more comebacks than Johnny Farnham, ok back to the #TheVoiceAu                    \n",
       "326286656854454273  *sigh* oh Colin ? #MKR                                                                                                     \n",
       "381988216292655104  RT @brian_day15: I swear, I'm not sexist, but I honestly just cannot stand the woman college football announcer on ESPN2   \n",
       "\n",
       "                     label  \n",
       "ID                          \n",
       "319399851215433729  sexism  \n",
       "320817818222358529  none    \n",
       "324114200450437120  none    \n",
       "326286656854454273  none    \n",
       "381988216292655104  sexism  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_links(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ', ')    \n",
    "    return text\n",
    "\n",
    "#code source is http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "\n",
    "import re\n",
    "cList = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"i'm\": \"i am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you will\",\n",
    "  \"you'll've\": \"you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expand_contractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    text = c_re.sub(replace, text.lower())\n",
    "    return text\n",
    "\n",
    "def strip_mentions_hashtags(text):\n",
    "    entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def high_precision(text):\n",
    "    for token in text:\n",
    "        words = text.split()\n",
    "        assert words.isLower() == word.isUpper()\n",
    "        \n",
    "        \n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    #text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_short\n",
    "\n",
    "# data cleaning and processing\n",
    "df['data'] = df['data'].apply(lambda x : strip_links(x))\n",
    "df['data'] = df['data'].apply(lambda x : expand_contractions(x))\n",
    "df['data'] = df['data'].apply(lambda x : strip_mentions_hashtags(x))\n",
    "df['data'] = df['data'].apply(lambda x : remove_special_characters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>319399851215433729</th>\n",
       "      <td>oh yeah colin smash those girls d</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320817818222358529</th>\n",
       "      <td>it is insane they keep bringing people back when will this show end</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324114200450437120</th>\n",
       "      <td>eagle this shit show has more comebacks than johnny farnham ok back to the</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326286656854454273</th>\n",
       "      <td>sigh oh colin</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381988216292655104</th>\n",
       "      <td>rt day15 i swear i am not sexist but i honestly just cannot stand the woman college football announcer on espn2</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381989167564996609</th>\n",
       "      <td>science is only good when it says men cannot be monogamous or some shit amirite</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381989237072990208</th>\n",
       "      <td>rt call me sexist but i think some women are seriously lacking knowledge l when it comes to feminism</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381989256404533251</th>\n",
       "      <td>rt call me sexist but females really need to stop acting like they are big football fans just stop</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381989513691529216</th>\n",
       "      <td>rt the mine is no place for a woman to work call me sexist but a woman as a rock driller is not practical</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382350557957984256</th>\n",
       "      <td>rt naija and do not call me sexist for that last tweet if women want equal praise for success they gotta take equal blame for failur</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382350622411874305</th>\n",
       "      <td>rt i am sorry but i do not see what is hot about girls who eat like guys call me sexist but i like girls because they are girls</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382351113204154368</th>\n",
       "      <td>every day i think maybe it will be the day i find no sexist tweets but it is never that day keep up the good fight women</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382518137582661632</th>\n",
       "      <td>rt but honestly stereotypes are light hearted and usually harmless let people joke around</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382518148617863168</th>\n",
       "      <td>rt feminism is a respectable ideal being over sensitive and searching for problems is not</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382518158470311936</th>\n",
       "      <td>rt i would not be so mad if not for the fact that i do not appreciate being called a bigot for saying that some t</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382518676865294336</th>\n",
       "      <td>rt that is a sexist tweet why just women sexism can work both ways</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382518915454087168</th>\n",
       "      <td>sexism can work both ways i can stop for today i got misogynist bingo before 9 a m</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382606225520615424</th>\n",
       "      <td>rt if you look at you see tweets of pure hatred if you look at you see a few light stupidity a</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382606809715834880</th>\n",
       "      <td>educate yourself on oppression</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382868890306363392</th>\n",
       "      <td>just read the phrase feminist agenda and snorted into my cereal yes let the international conspiracy be unveiled</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383627629766508545</th>\n",
       "      <td>rt i would rather have a blind dog as a ref than freakin liz i am sorry i am not sexist but that is why girls should not re</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383627770506399744</th>\n",
       "      <td>rt i am not sexist but women should play football past the age of 12 unless they are over 285</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383627997950910464</th>\n",
       "      <td>so true all women over 12 should play football but repeated concussions might make us as stupid as you</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383698416171118593</th>\n",
       "      <td>rt well i do not followyou you so stop trolling you have tohave something better to do then to look up sexist tw</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383698818698457089</th>\n",
       "      <td>oh i do plenty but picking rotten low hanging sexist fruit does not take up much of my day</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383699280130617345</th>\n",
       "      <td>rt abel i am not sexist at all but the saying men do not belong in the kitchen is utterly true i just tried making instant oa</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383730558863290368</th>\n",
       "      <td>rt okay fuck off my shit pussy</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383730982513168384</th>\n",
       "      <td>the sexist litmus test is if you call out a lvl 2 sexist comment amp the dood comes back with lvl 10 guns ablazing pussy amp cunt amp bitch</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383731135668162560</th>\n",
       "      <td>because femininity is so horrible i am not sexist but if a dude cries because of a girl in a wedding dress then he has a vagina</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384042596680474624</th>\n",
       "      <td>rt kell leave the washing to mum because she is not a complete remtard</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576761231819718656</th>\n",
       "      <td>at least i am assuming that is what it was i read 2 paragraphs got bored and deleted the message do not know the guy</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576761315756101632</th>\n",
       "      <td>which is why the joint blacks and muslims are murdering police in ferguson</td>\n",
       "      <td>racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576764046411632640</th>\n",
       "      <td>ahh this is a christian conspiracy book involving so few real christians that their actions are unnoticed and irrelevant</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576764285314957312</th>\n",
       "      <td>i know hundreds of christians but not a single one that fits the discription</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576765290299568128</th>\n",
       "      <td>so while thousands of churches temples synagogs have been destroyed by muslim biggots you only worry about one muslim</td>\n",
       "      <td>racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576765492448268288</th>\n",
       "      <td>mosque that is still standing typical muslim terrorist asshole</td>\n",
       "      <td>racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576767285387354113</th>\n",
       "      <td>max looks at the genocide of 7000 ezidis and countless christians by his muslims friends and writes not a word</td>\n",
       "      <td>racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576768402091446272</th>\n",
       "      <td>but who can blame him with his 24 7 schedule of promoting anti semitism</td>\n",
       "      <td>racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576770361275990016</th>\n",
       "      <td>rt state dept to shut down part of an email network believed to be infected by russian hackers</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576770673550323712</th>\n",
       "      <td>putin has literally thousands of hired trolls working the internet to shape opinion with putin propaganda</td>\n",
       "      <td>racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576771329975664640</th>\n",
       "      <td>rt dear state department where is hillary clintons of 109 form</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576772044362158080</th>\n",
       "      <td>castro safar filthy islam created them and is funding them</td>\n",
       "      <td>racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576772176457547778</th>\n",
       "      <td>castro safar</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576773965529550848</th>\n",
       "      <td>and let us not forget that epstein is in the mold of the american left and great friends with the clintons</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576777529471569920</th>\n",
       "      <td>max s palestinian muslim friends throw an atheist blogger into prison for 10 months for his religious views</td>\n",
       "      <td>racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576778079386791936</th>\n",
       "      <td>why not muslims have practically exterminated every non muslim minority in mus countries</td>\n",
       "      <td>racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576778967320911872</th>\n",
       "      <td>castro safar your goat is calling you terrorist she is horny</td>\n",
       "      <td>racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576779016432013312</th>\n",
       "      <td>castro safar</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576780193047584768</th>\n",
       "      <td>there is nothing incorrect about the generalization look at minority demographics in</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576780412564848640</th>\n",
       "      <td>in any majority muslim country</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576780636930777088</th>\n",
       "      <td>and there is nothing wrong with stoping the muslim genocide of non muslims</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576781307612651521</th>\n",
       "      <td>castro safar kkk is almost non existent but most muslims are terrorists</td>\n",
       "      <td>racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576781790334988288</th>\n",
       "      <td>they are a minority in india microbrain next door in pakistan muslim mobs are murdering</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576781938339356672</th>\n",
       "      <td>hindus and christians for blasphemy</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588296672226058240</th>\n",
       "      <td>oh fuck me hard with a rusty chainsaw another round of instant restaurants</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590818760337924096</th>\n",
       "      <td>omg shut up drasko and bianca</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592273294562209793</th>\n",
       "      <td>stfu drasko</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595166563516329985</th>\n",
       "      <td>ash found her inner bogun ewwww</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618367060117098498</th>\n",
       "      <td>sorry but on is being a real cunt you are not all that</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684779048120258560</th>\n",
       "      <td>chloe and kelly i seriously hate those girls arrogant</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16135 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                           data  \\\n",
       "ID                                                                                                                                                                \n",
       "319399851215433729  oh yeah colin smash those girls d                                                                                                             \n",
       "320817818222358529  it is insane they keep bringing people back when will this show end                                                                           \n",
       "324114200450437120  eagle this shit show has more comebacks than johnny farnham ok back to the                                                                    \n",
       "326286656854454273  sigh oh colin                                                                                                                                 \n",
       "381988216292655104  rt day15 i swear i am not sexist but i honestly just cannot stand the woman college football announcer on espn2                               \n",
       "381989167564996609  science is only good when it says men cannot be monogamous or some shit amirite                                                               \n",
       "381989237072990208  rt call me sexist but i think some women are seriously lacking knowledge l when it comes to feminism                                          \n",
       "381989256404533251  rt call me sexist but females really need to stop acting like they are big football fans just stop                                            \n",
       "381989513691529216  rt the mine is no place for a woman to work call me sexist but a woman as a rock driller is not practical                                     \n",
       "382350557957984256  rt naija and do not call me sexist for that last tweet if women want equal praise for success they gotta take equal blame for failur          \n",
       "382350622411874305  rt i am sorry but i do not see what is hot about girls who eat like guys call me sexist but i like girls because they are girls               \n",
       "382351113204154368  every day i think maybe it will be the day i find no sexist tweets but it is never that day keep up the good fight women                      \n",
       "382518137582661632  rt but honestly stereotypes are light hearted and usually harmless let people joke around                                                     \n",
       "382518148617863168  rt feminism is a respectable ideal being over sensitive and searching for problems is not                                                     \n",
       "382518158470311936  rt i would not be so mad if not for the fact that i do not appreciate being called a bigot for saying that some t                             \n",
       "382518676865294336  rt that is a sexist tweet why just women sexism can work both ways                                                                            \n",
       "382518915454087168  sexism can work both ways i can stop for today i got misogynist bingo before 9 a m                                                            \n",
       "382606225520615424  rt if you look at you see tweets of pure hatred if you look at you see a few light stupidity a                                                \n",
       "382606809715834880  educate yourself on oppression                                                                                                                \n",
       "382868890306363392  just read the phrase feminist agenda and snorted into my cereal yes let the international conspiracy be unveiled                              \n",
       "383627629766508545  rt i would rather have a blind dog as a ref than freakin liz i am sorry i am not sexist but that is why girls should not re                   \n",
       "383627770506399744  rt i am not sexist but women should play football past the age of 12 unless they are over 285                                                 \n",
       "383627997950910464  so true all women over 12 should play football but repeated concussions might make us as stupid as you                                        \n",
       "383698416171118593  rt well i do not followyou you so stop trolling you have tohave something better to do then to look up sexist tw                              \n",
       "383698818698457089  oh i do plenty but picking rotten low hanging sexist fruit does not take up much of my day                                                    \n",
       "383699280130617345  rt abel i am not sexist at all but the saying men do not belong in the kitchen is utterly true i just tried making instant oa                 \n",
       "383730558863290368  rt okay fuck off my shit pussy                                                                                                                \n",
       "383730982513168384  the sexist litmus test is if you call out a lvl 2 sexist comment amp the dood comes back with lvl 10 guns ablazing pussy amp cunt amp bitch   \n",
       "383731135668162560  because femininity is so horrible i am not sexist but if a dude cries because of a girl in a wedding dress then he has a vagina               \n",
       "384042596680474624  rt kell leave the washing to mum because she is not a complete remtard                                                                        \n",
       "...                                                                                                                                             ...               \n",
       "576761231819718656  at least i am assuming that is what it was i read 2 paragraphs got bored and deleted the message do not know the guy                          \n",
       "576761315756101632  which is why the joint blacks and muslims are murdering police in ferguson                                                                    \n",
       "576764046411632640  ahh this is a christian conspiracy book involving so few real christians that their actions are unnoticed and irrelevant                      \n",
       "576764285314957312  i know hundreds of christians but not a single one that fits the discription                                                                  \n",
       "576765290299568128  so while thousands of churches temples synagogs have been destroyed by muslim biggots you only worry about one muslim                         \n",
       "576765492448268288  mosque that is still standing typical muslim terrorist asshole                                                                                \n",
       "576767285387354113  max looks at the genocide of 7000 ezidis and countless christians by his muslims friends and writes not a word                                \n",
       "576768402091446272  but who can blame him with his 24 7 schedule of promoting anti semitism                                                                       \n",
       "576770361275990016  rt state dept to shut down part of an email network believed to be infected by russian hackers                                                \n",
       "576770673550323712  putin has literally thousands of hired trolls working the internet to shape opinion with putin propaganda                                     \n",
       "576771329975664640  rt dear state department where is hillary clintons of 109 form                                                                                \n",
       "576772044362158080  castro safar filthy islam created them and is funding them                                                                                    \n",
       "576772176457547778  castro safar                                                                                                                                  \n",
       "576773965529550848  and let us not forget that epstein is in the mold of the american left and great friends with the clintons                                    \n",
       "576777529471569920  max s palestinian muslim friends throw an atheist blogger into prison for 10 months for his religious views                                   \n",
       "576778079386791936  why not muslims have practically exterminated every non muslim minority in mus countries                                                      \n",
       "576778967320911872  castro safar your goat is calling you terrorist she is horny                                                                                  \n",
       "576779016432013312  castro safar                                                                                                                                  \n",
       "576780193047584768  there is nothing incorrect about the generalization look at minority demographics in                                                          \n",
       "576780412564848640  in any majority muslim country                                                                                                                \n",
       "576780636930777088  and there is nothing wrong with stoping the muslim genocide of non muslims                                                                    \n",
       "576781307612651521  castro safar kkk is almost non existent but most muslims are terrorists                                                                       \n",
       "576781790334988288  they are a minority in india microbrain next door in pakistan muslim mobs are murdering                                                       \n",
       "576781938339356672  hindus and christians for blasphemy                                                                                                           \n",
       "588296672226058240  oh fuck me hard with a rusty chainsaw another round of instant restaurants                                                                    \n",
       "590818760337924096  omg shut up drasko and bianca                                                                                                                 \n",
       "592273294562209793  stfu drasko                                                                                                                                   \n",
       "595166563516329985  ash found her inner bogun ewwww                                                                                                               \n",
       "618367060117098498  sorry but on is being a real cunt you are not all that                                                                                        \n",
       "684779048120258560  chloe and kelly i seriously hate those girls arrogant                                                                                         \n",
       "\n",
       "                     label  \n",
       "ID                          \n",
       "319399851215433729  sexism  \n",
       "320817818222358529  none    \n",
       "324114200450437120  none    \n",
       "326286656854454273  none    \n",
       "381988216292655104  sexism  \n",
       "381989167564996609  none    \n",
       "381989237072990208  sexism  \n",
       "381989256404533251  sexism  \n",
       "381989513691529216  sexism  \n",
       "382350557957984256  sexism  \n",
       "382350622411874305  sexism  \n",
       "382351113204154368  none    \n",
       "382518137582661632  sexism  \n",
       "382518148617863168  sexism  \n",
       "382518158470311936  sexism  \n",
       "382518676865294336  sexism  \n",
       "382518915454087168  none    \n",
       "382606225520615424  sexism  \n",
       "382606809715834880  none    \n",
       "382868890306363392  none    \n",
       "383627629766508545  sexism  \n",
       "383627770506399744  sexism  \n",
       "383627997950910464  sexism  \n",
       "383698416171118593  sexism  \n",
       "383698818698457089  sexism  \n",
       "383699280130617345  sexism  \n",
       "383730558863290368  sexism  \n",
       "383730982513168384  sexism  \n",
       "383731135668162560  sexism  \n",
       "384042596680474624  sexism  \n",
       "...                    ...  \n",
       "576761231819718656  none    \n",
       "576761315756101632  racism  \n",
       "576764046411632640  none    \n",
       "576764285314957312  none    \n",
       "576765290299568128  racism  \n",
       "576765492448268288  racism  \n",
       "576767285387354113  racism  \n",
       "576768402091446272  racism  \n",
       "576770361275990016  none    \n",
       "576770673550323712  racism  \n",
       "576771329975664640  none    \n",
       "576772044362158080  racism  \n",
       "576772176457547778  none    \n",
       "576773965529550848  none    \n",
       "576777529471569920  racism  \n",
       "576778079386791936  racism  \n",
       "576778967320911872  racism  \n",
       "576779016432013312  none    \n",
       "576780193047584768  none    \n",
       "576780412564848640  none    \n",
       "576780636930777088  none    \n",
       "576781307612651521  racism  \n",
       "576781790334988288  none    \n",
       "576781938339356672  none    \n",
       "588296672226058240  none    \n",
       "590818760337924096  none    \n",
       "592273294562209793  none    \n",
       "595166563516329985  sexism  \n",
       "618367060117098498  sexism  \n",
       "684779048120258560  sexism  \n",
       "\n",
       "[16135 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting dataset into train and validation set (80% - 20% default)\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['data'], df['label'])\n",
    "\n",
    "\n",
    "#Label Encoder converts yes and no into 1 and 0\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "#Count vectorizer will calculate count of every word in text data and will ignore number and whitespaces\n",
    "\n",
    "count_vector = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vector.fit(df['data'])\n",
    "\n",
    "#using the count vector defined above, we'll transform our existing text data into train_x_count where every row will indicate \n",
    "#tweet and every column will represent count of word indexed at that loc\n",
    "\n",
    "train_x_count = count_vector.transform(train_x)\n",
    "valid_x_count = count_vector.transform(valid_x)\n",
    "\n",
    "#word level tf-idf\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(df['data'])\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_x)\n",
    "\n",
    "\n",
    "#ngram level tf-idf\n",
    "tfidf_ngram = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}', ngram_range=(2,3) ,max_features=5000)\n",
    "tfidf_ngram.fit(df['data'])\n",
    "xtrain_tfidf_ngram = tfidf_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram = tfidf_ngram.transform(valid_x)\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(df['data'])\n",
    "word_index = token.word_index\n",
    "\n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=1000)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import sys,json,math\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "def load_word2vec(filename):\n",
    "    # Returns a dict containing a {word: numpy array for a dense word vector} mapping.\n",
    "    # It loads everything into memory.\n",
    "    \n",
    "    w2vec={}\n",
    "    with open(filename,\"r\") as f_in:\n",
    "        for line in f_in:\n",
    "            line_split=line.replace(\"\\n\",\"\").split()\n",
    "            w=line_split[0]\n",
    "            vec=np.array([float(x) for x in line_split[1:]])\n",
    "            w2vec[w]=vec\n",
    "    return w2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "('Done.', 1193514, ' words loaded!')\n"
     ]
    }
   ],
   "source": [
    "word_to_vec_dict = loadGloveModel('glove.twitter.27B.200d.txt')\n",
    "\n",
    "import numpy\n",
    "emb_matrix = numpy.zeros((len(word_index)+1, 200))\n",
    "for word, i in word_index.items():\n",
    "    emb_vect = word_to_vec_dict.get(word)\n",
    "    if emb_vect is not None:\n",
    "        emb_matrix[i] = emb_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.DataFrame(train_x)\n",
    "dataset_valid = pd.DataFrame(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_topic_model = pd.concat([dataset_train, dataset_valid], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dataset_for_topic_model['data'])\n",
    "lda = LatentDirichletAllocation(random_state=0)\n",
    "doc_topic_matrix = lda.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train['comp'] = dataset_train['data'].apply(lambda x : analyser.polarity_scores(x)['compound'])\n",
    "dataset_train['pos'] = dataset_train['data'].apply(lambda x : analyser.polarity_scores(x)['pos'])\n",
    "dataset_train['neu'] = dataset_train['data'].apply(lambda x : analyser.polarity_scores(x)['neu'])\n",
    "dataset_train['neg'] = dataset_train['data'].apply(lambda x : analyser.polarity_scores(x)['neg'])\n",
    "\n",
    "dataset_valid['comp'] = dataset_valid['data'].apply(lambda x : analyser.polarity_scores(x)['compound'])\n",
    "dataset_valid['pos'] = dataset_valid['data'].apply(lambda x : analyser.polarity_scores(x)['pos'])\n",
    "dataset_valid['neu'] = dataset_valid['data'].apply(lambda x : analyser.polarity_scores(x)['neu'])\n",
    "dataset_valid['neg'] = dataset_valid['data'].apply(lambda x : analyser.polarity_scores(x)['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine sentiment features with count vectorizer features\n",
    "train_x_count = pd.DataFrame(np.concatenate((train_x_count.toarray(), dataset_train[['comp', 'pos', 'neu', 'neg']].values, doc_topic_matrix[:len(train_x)]), axis=1))\n",
    "valid_x_count = pd.DataFrame(np.concatenate((valid_x_count.toarray(), dataset_valid[['comp', 'pos', 'neu', 'neg']].values, doc_topic_matrix[len(train_x):]), axis=1))\n",
    "\n",
    "# combine sentiment features with tf-idf features\n",
    "xtrain_tfidf = pd.DataFrame(np.concatenate((xtrain_tfidf.toarray(), dataset_train[['comp', 'pos', 'neu', 'neg']].values, doc_topic_matrix[:len(train_x)]), axis=1))\n",
    "xvalid_tfidf = pd.DataFrame(np.concatenate((xvalid_tfidf.toarray(), dataset_valid[['comp', 'pos', 'neu', 'neg']].values, doc_topic_matrix[len(train_x):]), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_model(classifier, feature_vector_train,label, feature_vector_valid, is_neural_net = False):\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    prediction = classifier.predict(feature_vector_valid)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    if not is_neural_net:\n",
    "        print(confusion_matrix(valid_y, prediction))\n",
    "    if is_neural_net:\n",
    "        prediction = prediction.argmax(axis=-1)\n",
    "    return metrics.accuracy_score(prediction, valid_y), f1_score(prediction, valid_y,average=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2307  169  153]\n",
      " [ 135  346    3]\n",
      " [ 345    4  450]]\n",
      "('Naive Bayes (Count Vectors)', 0.793200408997955, array([0.85192024, 0.68993021, 0.6405694 ]))\n",
      "[[2568   36   25]\n",
      " [ 294  190    0]\n",
      " [ 551    0  248]]\n",
      "('Naive Bayes (TDIDF)', 0.7684049079754601, array([0.85004965, 0.53521127, 0.46268657]))\n"
     ]
    }
   ],
   "source": [
    "count_vector = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vector.fit(df['data'])\n",
    "nb_train_x_count = count_vector.transform(train_x)\n",
    "nb_valid_x_count = count_vector.transform(valid_x)\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(df['data'])\n",
    "nb_xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "nb_xvalid_tfidf = tfidf_vect.transform(valid_x)\n",
    "n\n",
    "\n",
    "\n",
    "clf = naive_bayes.MultinomialNB()\n",
    "accuracy, f1 = train_model(clf, nb_train_x_count, train_y, nb_valid_x_count)\n",
    "print(\"Naive Bayes (Count Vectors)\",accuracy, f1)\n",
    "accuracy, f1 = train_model(clf, nb_xtrain_tfidf, train_y, nb_xvalid_tfidf)\n",
    "print(\"Naive Bayes (TDIDF)\", accuracy, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][[2434   86  113]\n",
      " [ 149  334    4]\n",
      " [ 318    2  472]]\n",
      "('LR using CV', 0.8282208588957055, array([0.87965305, 0.73487349, 0.68356264]))\n",
      "[LibLinear][[2495   68   70]\n",
      " [ 207  275    5]\n",
      " [ 417    0  375]]\n",
      "('LR using CV', 0.8039366053169734, array([0.86752434, 0.6626506 , 0.60386473]))\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.LogisticRegression(verbose=1)\n",
    "accuracy, f1 = train_model(clf, train_x_count, train_y, valid_x_count)\n",
    "print(\"LR using CV\", accuracy, f1)\n",
    "accuracy, f1 = train_model(clf, xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"LR using CV\", accuracy, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1 = train_model(xgboost.XGBClassifier(verbose=1), train_x_count, train_y, valid_x_count)\n",
    "print(\"xgboost using CV\", accuracy, f1)\n",
    "accuracy, f1 = train_model(xgboost.XGBClassifier(verbose=1), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"xgboost using TFIDF\", accuracy, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shallow Neural Networks\n",
    "\n",
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11736/11736 [==============================] - 5s 419us/step - loss: 0.4249\n",
      "Epoch 2/20\n",
      "11736/11736 [==============================] - 4s 361us/step - loss: -0.4221\n",
      "Epoch 3/20\n",
      "11736/11736 [==============================] - 4s 378us/step - loss: -0.7532\n",
      "Epoch 4/20\n",
      "11736/11736 [==============================] - 4s 343us/step - loss: -0.9797\n",
      "Epoch 5/20\n",
      "11736/11736 [==============================] - 4s 339us/step - loss: -1.1788\n",
      "Epoch 6/20\n",
      "11736/11736 [==============================] - 4s 336us/step - loss: -1.3251\n",
      "Epoch 7/20\n",
      "11736/11736 [==============================] - 4s 338us/step - loss: -1.4424\n",
      "Epoch 8/20\n",
      "11736/11736 [==============================] - 4s 339us/step - loss: -1.5396\n",
      "Epoch 9/20\n",
      "11736/11736 [==============================] - 4s 338us/step - loss: -1.6192\n",
      "Epoch 10/20\n",
      "11736/11736 [==============================] - 4s 337us/step - loss: -1.6858\n",
      "Epoch 11/20\n",
      "11736/11736 [==============================] - 4s 337us/step - loss: -1.7446\n",
      "Epoch 12/20\n",
      "11736/11736 [==============================] - 4s 336us/step - loss: -1.7928\n",
      "Epoch 13/20\n",
      "11736/11736 [==============================] - 4s 339us/step - loss: -1.8363\n",
      "Epoch 14/20\n",
      "11736/11736 [==============================] - 4s 338us/step - loss: -1.8740\n",
      "Epoch 15/20\n",
      "11736/11736 [==============================] - 4s 338us/step - loss: -1.9151\n",
      "Epoch 16/20\n",
      "11736/11736 [==============================] - 4s 337us/step - loss: -1.9417\n",
      "Epoch 17/20\n",
      "11736/11736 [==============================] - 4s 336us/step - loss: -1.9786\n",
      "Epoch 18/20\n",
      "11736/11736 [==============================] - 4s 335us/step - loss: -2.0036\n",
      "Epoch 19/20\n",
      "11736/11736 [==============================] - 4s 345us/step - loss: -2.0292\n",
      "Epoch 20/20\n",
      "11736/11736 [==============================] - 4s 345us/step - loss: -2.0578\n",
      "('NN, Ngram Level TF IDF Vectors', 0.6730572597137015)\n"
     ]
    }
   ],
   "source": [
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "accuracy, f1 = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "print (\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "11736/11736 [==============================] - 5s 415us/step - loss: 0.4216\n",
      "Epoch 2/2\n",
      "11736/11736 [==============================] - 4s 339us/step - loss: -0.4022\n",
      "('NN, Ngram Level TF IDF Vectors', 0.6730572597137015)\n"
     ]
    }
   ],
   "source": [
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "accuracy, f1 = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "print (\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Epoch 1/2\n",
      "11736/11736 [==============================] - 56s 5ms/step - loss: 0.0461\n",
      "Epoch 2/2\n",
      "11736/11736 [==============================] - 54s 5ms/step - loss: -0.3812\n",
      "(0.6730572597137015, array([0.80458365, 0.        , 0.        ]))\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_gru():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 200, weights=[emb_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the GRU Layer\n",
    "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_gru()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Add layers to model\n",
    "model.add(Dense(100, activation='relu', input_shape = (70,)))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_11_input to have shape (1000,) but got array with shape (70,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5fba8e197ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#train_seq_x, train_y, valid_seq_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages/keras/engine/training_utils.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_11_input to have shape (1000,) but got array with shape (70,)"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(patience=2) \n",
    "#train_seq_x, train_y, valid_seq_x\n",
    "#fit model\n",
    "model.fit(train_seq_x, train_y, validation_split=0.3, epochs=20, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/i504969/anaconda3/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10562 samples, validate on 1174 samples\n",
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "max_words = 1000\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "y_train = utils.to_categorical(train_y, 3)\n",
    "y_test = utils.to_categorical(valid_y, 3)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit(train_seq_x, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=3,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_class = []\n",
    "\n",
    "for i in df.Class:\n",
    "    if i == 'sexism':\n",
    "        new_class.append(0)\n",
    "    elif i == 'racism':\n",
    "        new_class.append(1)\n",
    "    else:\n",
    "        new_class.append(2)\n",
    "\n",
    "#print(list(df.Class)[0:10])\n",
    "#print(new_class[0:10])\n",
    "\n",
    "target=to_categorical(new_class)\n",
    "n_cols = predictors.shape[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
