{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ULMFiT Models",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsMgf8LLsoOq",
        "colab_type": "text"
      },
      "source": [
        "Let's install PyTorch and fastai libraries first. You have to repeat this step every time you restart this notebook in colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L-mYNWDBB4g",
        "colab_type": "code",
        "outputId": "fba3f59a-fbe2-4ed5-bbcd-cf100fd140f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "source": [
        "!pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n",
        "!pip install fastai"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n",
            "Collecting torch_nightly\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/nightly/cu92/torch_nightly-1.2.0.dev20190805%2Bcu92-cp36-cp36m-linux_x86_64.whl (704.8MB)\n",
            "\u001b[K     |████████████████████████████████| 704.8MB 20kB/s \n",
            "\u001b[?25hInstalling collected packages: torch-nightly\n",
            "Successfully installed torch-nightly-1.2.0.dev20190805+cu92\n",
            "Requirement already satisfied: fastai in /usr/local/lib/python3.6/dist-packages (1.0.57)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai) (2.21.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai) (3.13)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai) (1.3.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai) (2.6.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai) (3.0.3)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.16.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai) (0.6)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai) (7.352.0)\n",
            "Requirement already satisfied: typing; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai) (3.7.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai) (4.6.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai) (0.24.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai) (19.1)\n",
            "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from fastai) (2.1.8)\n",
            "Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.6/dist-packages (from fastai) (0.1.21)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai) (0.3.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai) (4.3.0)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai) (1.2.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2019.6.16)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (2.4.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2018.9)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (19.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (1.12.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.0.7)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.1)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.2.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.9.6)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (7.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.2)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->fastai) (0.46)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->fastai) (41.0.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy>=2.0.18->fastai) (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA87AbrrBigr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import libraries\n",
        "import fastai\n",
        "from fastai import *\n",
        "from fastai.text import * \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "import io\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fp3_q5DiblR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hs = pd.read_csv('hatespeech.csv', encoding=\"latin-1\",index_col=6, keep_default_na=False)\n",
        "#print(hs.head())\n",
        "\n",
        "orig = pd.read_csv('NAACL_SRW_2016.csv', index_col=0, header=None,encoding='latin-1')\n",
        "orig.index.name = 'ID'\n",
        "orig = orig.rename(columns={1: 'Class'})\n",
        "orig.index = orig.index.astype(str)\n",
        "#print(orig.head())\n",
        "\n",
        "#merging the two dataframes\n",
        "hs = pd.merge(hs, orig, how='inner', left_index=True, right_index=True)\n",
        "#print(hs.head())\n",
        "df = hs\n",
        "from sklearn import model_selection, preprocessing\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1cr6P3DJMG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_minority_racism = df[df['Class']=='racism']\n",
        "df_minority_sexism = df[df['Class']=='sexism']\n",
        "df_majority_neu =  df[df['Class']=='none']\n",
        "from sklearn.utils import resample\n",
        "\n",
        "df_minority_upsampled = resample(df_minority_sexism, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=5000,    # to match majority class\n",
        "                                 random_state=500) # reproducible results\n",
        "df_minority_upsampled_racism = resample(df_minority_racism, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=5000,    # to match majority class\n",
        "                                 random_state=500) # reproducible results\n",
        "\n",
        "df = pd.concat([df_majority_neu, df_minority_upsampled_racism, df_minority_upsampled])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g0G3bJ5JTl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = preprocessing.LabelEncoder()\n",
        "df['Class'] = encoder.fit_transform(df['Class'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9SHCfiABlcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "dataset = df\n",
        "documents = dataset.Tweets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3zbA9rfCfRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a dataframe\n",
        "df = pd.DataFrame({'label':dataset['Class'],\n",
        "                   'text':dataset.Tweets})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK_i0X2kD6AC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split data into training and validation set\n",
        "df_trn, df_val = train_test_split(df, stratify = df['label'], test_size = 0.4, random_state = 12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYEFCrJOEHSG",
        "colab_type": "code",
        "outputId": "70a2e656-184d-4de3-da7e-fe86f409d485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df_trn.shape, df_val.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((12619, 2), (8414, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvvFWWM4ljoH",
        "colab_type": "text"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tNP_aMTEOKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Language model data\n",
        "data_lm = TextLMDataBunch.from_df(train_df = df_trn, valid_df = df_val, path = \"\")\n",
        "\n",
        "# Classifier model data\n",
        "data_clas = TextClasDataBunch.from_df(path = \"\", train_df = df_trn, valid_df = df_val, vocab=data_lm.train_ds.vocab, bs=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ua8pmHEEP5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = language_model_learner(data_lm, arch=AWD_LSTM, drop_mult=0.4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE3likAPETlz",
        "colab_type": "code",
        "outputId": "dfc8f305-c555-4a16-f32a-a381e7dcf16f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "# train the learner object\n",
        "learn.fit_one_cycle(1, 1e-2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.668283</td>\n",
              "      <td>3.990028</td>\n",
              "      <td>0.304678</td>\n",
              "      <td>00:16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3eJMLWDKb9u",
        "colab_type": "code",
        "outputId": "77f77a3f-60f0-47d4-9c6b-12ee389bbc0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81fA6MpQKpJw",
        "colab_type": "code",
        "outputId": "fb6c4a37-c919-4674-90e0-839d10d48887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "learn.recorder.plot()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3v8H95F6S8l3AGIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7fO0cuVET8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save_encoder('ft_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSkXI7HJET_r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67898f1b-690f-486f-eba6-ad46da81e938"
      },
      "source": [
        "learn = text_classifier_learner(data_clas,arch= AWD_LSTM, drop_mult=0.3)\n",
        "learn.load_encoder('ft_enc')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (12619 items)\n",
              "x: TextList\n",
              "xxbos @tintinenameriq @maxblumenthal @campagnebds xxmaj shows how stupid you 've become . xxmaj only xxmaj muslim terrorists actually attack talks .,xxbos @98halima @johnnygjokaj @bilalighumman @cdnkhadija @rfrankh53 xxup isis wants to control the world because xxmaj mohammed wanted to control it .,xxbos xxmaj he is not all there . xxmaj and i guess he keeps forgetting that the dog just went outside and ug xxrep 4 h i want to sleep .,xxbos xxmaj oh $ xxunk , xxmaj psycho xxmaj annie is back . > xxunk # xxup mkr,xxbos xxmaj the sexist litmus test is if you call out a lvl 2 sexist comment & & the dood comes back with lvl 10 guns ablazing , \" pussy \" & & \" cunt \" & & \" bitch \"\n",
              "y: CategoryList\n",
              "1,1,0,0,2\n",
              "Path: .;\n",
              "\n",
              "Valid: LabelList (8414 items)\n",
              "x: TextList\n",
              "xxbos xxup rt http : / / t.co / xxunk xxunk xxmaj call me sexist but men will always be superior drivers,xxbos xxup rt @shermertron : xxup @mt8_9 xxunk xxmaj the xxmaj democratic xxmaj people 's xxmaj xxunk of xxmaj korea must be a wonderful place , right ? xxmaj look up the words ! ! !,xxbos # mkr,xxbos xxunk and what a xxunk ass you are . xxmaj kindly go fuck yourself . :),xxbos @farooqsumar @nafeezahmed @maxblumenthal xxmaj in fact the pedophile prophet clearly stated his intention in xxmaj hadiths . http : / / t.co / xxunk\n",
              "y: CategoryList\n",
              "2,2,0,0,1\n",
              "Path: .;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(9576, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(9576, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.12, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=3, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f9fc643ad08>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
              "learn: RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (12619 items)\n",
              "x: TextList\n",
              "xxbos @tintinenameriq @maxblumenthal @campagnebds xxmaj shows how stupid you 've become . xxmaj only xxmaj muslim terrorists actually attack talks .,xxbos @98halima @johnnygjokaj @bilalighumman @cdnkhadija @rfrankh53 xxup isis wants to control the world because xxmaj mohammed wanted to control it .,xxbos xxmaj he is not all there . xxmaj and i guess he keeps forgetting that the dog just went outside and ug xxrep 4 h i want to sleep .,xxbos xxmaj oh $ xxunk , xxmaj psycho xxmaj annie is back . > xxunk # xxup mkr,xxbos xxmaj the sexist litmus test is if you call out a lvl 2 sexist comment & & the dood comes back with lvl 10 guns ablazing , \" pussy \" & & \" cunt \" & & \" bitch \"\n",
              "y: CategoryList\n",
              "1,1,0,0,2\n",
              "Path: .;\n",
              "\n",
              "Valid: LabelList (8414 items)\n",
              "x: TextList\n",
              "xxbos xxup rt http : / / t.co / xxunk xxunk xxmaj call me sexist but men will always be superior drivers,xxbos xxup rt @shermertron : xxup @mt8_9 xxunk xxmaj the xxmaj democratic xxmaj people 's xxmaj xxunk of xxmaj korea must be a wonderful place , right ? xxmaj look up the words ! ! !,xxbos # mkr,xxbos xxunk and what a xxunk ass you are . xxmaj kindly go fuck yourself . :),xxbos @farooqsumar @nafeezahmed @maxblumenthal xxmaj in fact the pedophile prophet clearly stated his intention in xxmaj hadiths . http : / / t.co / xxunk\n",
              "y: CategoryList\n",
              "2,2,0,0,1\n",
              "Path: .;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(9576, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(9576, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.12, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=3, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f9fc643ad08>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
              "  (0): Embedding(9576, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(9576, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.12, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=3, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False, cb_fns_registered=False)\n",
              "alpha: 2.0\n",
              "beta: 1.0], layer_groups=[Sequential(\n",
              "  (0): Embedding(9576, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(9576, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.12, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=3, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False, cb_fns_registered=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VaunwDAMmLV",
        "colab_type": "code",
        "outputId": "0a058b36-d5d7-4ffe-c544-273c8e9546bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "learn.lr_find()\n",
        "learn.recorder.plot()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQ0REAvL/i+e3AMIJwlAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FTRW9v7KVYt",
        "colab_type": "code",
        "outputId": "7721d15a-681a-4ffe-dc4b-f5a7a8460650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learn.fit_one_cycle(50, 1e-2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.623416</td>\n",
              "      <td>0.566332</td>\n",
              "      <td>0.777157</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.571242</td>\n",
              "      <td>0.510927</td>\n",
              "      <td>0.787259</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.576036</td>\n",
              "      <td>0.480368</td>\n",
              "      <td>0.795341</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.530852</td>\n",
              "      <td>0.471906</td>\n",
              "      <td>0.793677</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.544762</td>\n",
              "      <td>0.478435</td>\n",
              "      <td>0.797362</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.528051</td>\n",
              "      <td>0.460720</td>\n",
              "      <td>0.802115</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.523531</td>\n",
              "      <td>0.466797</td>\n",
              "      <td>0.792132</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.528650</td>\n",
              "      <td>0.470811</td>\n",
              "      <td>0.793440</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.537413</td>\n",
              "      <td>0.446670</td>\n",
              "      <td>0.806988</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.521645</td>\n",
              "      <td>0.465889</td>\n",
              "      <td>0.800214</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.522994</td>\n",
              "      <td>0.441288</td>\n",
              "      <td>0.809484</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.535162</td>\n",
              "      <td>0.447253</td>\n",
              "      <td>0.811267</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.507771</td>\n",
              "      <td>0.460571</td>\n",
              "      <td>0.807464</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.514213</td>\n",
              "      <td>0.456176</td>\n",
              "      <td>0.799144</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.504677</td>\n",
              "      <td>0.446846</td>\n",
              "      <td>0.806513</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.527038</td>\n",
              "      <td>0.439992</td>\n",
              "      <td>0.810078</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.489029</td>\n",
              "      <td>0.431105</td>\n",
              "      <td>0.817209</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.500139</td>\n",
              "      <td>0.436625</td>\n",
              "      <td>0.814714</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.521682</td>\n",
              "      <td>0.437394</td>\n",
              "      <td>0.812455</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.492437</td>\n",
              "      <td>0.442313</td>\n",
              "      <td>0.808533</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.467923</td>\n",
              "      <td>0.461916</td>\n",
              "      <td>0.797837</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.490464</td>\n",
              "      <td>0.421802</td>\n",
              "      <td>0.819586</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.468921</td>\n",
              "      <td>0.420860</td>\n",
              "      <td>0.824578</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.474682</td>\n",
              "      <td>0.423055</td>\n",
              "      <td>0.812337</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.469702</td>\n",
              "      <td>0.424203</td>\n",
              "      <td>0.820894</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.443500</td>\n",
              "      <td>0.410754</td>\n",
              "      <td>0.824222</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.456506</td>\n",
              "      <td>0.410458</td>\n",
              "      <td>0.829689</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.479919</td>\n",
              "      <td>0.404689</td>\n",
              "      <td>0.824816</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.440402</td>\n",
              "      <td>0.415791</td>\n",
              "      <td>0.821369</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.448451</td>\n",
              "      <td>0.403961</td>\n",
              "      <td>0.824816</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.432844</td>\n",
              "      <td>0.402325</td>\n",
              "      <td>0.826123</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.424742</td>\n",
              "      <td>0.398966</td>\n",
              "      <td>0.828619</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.440458</td>\n",
              "      <td>0.391507</td>\n",
              "      <td>0.838840</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.415183</td>\n",
              "      <td>0.389818</td>\n",
              "      <td>0.833016</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.430834</td>\n",
              "      <td>0.387645</td>\n",
              "      <td>0.840029</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.422840</td>\n",
              "      <td>0.395784</td>\n",
              "      <td>0.825410</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.426202</td>\n",
              "      <td>0.377839</td>\n",
              "      <td>0.838483</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.408265</td>\n",
              "      <td>0.384051</td>\n",
              "      <td>0.837652</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.409242</td>\n",
              "      <td>0.383066</td>\n",
              "      <td>0.837057</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.386044</td>\n",
              "      <td>0.383975</td>\n",
              "      <td>0.836582</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.408166</td>\n",
              "      <td>0.378544</td>\n",
              "      <td>0.840147</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.416155</td>\n",
              "      <td>0.380717</td>\n",
              "      <td>0.835988</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.363380</td>\n",
              "      <td>0.376864</td>\n",
              "      <td>0.837770</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.371741</td>\n",
              "      <td>0.377154</td>\n",
              "      <td>0.839672</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.390463</td>\n",
              "      <td>0.387868</td>\n",
              "      <td>0.835750</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.419870</td>\n",
              "      <td>0.376768</td>\n",
              "      <td>0.842405</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.371216</td>\n",
              "      <td>0.373451</td>\n",
              "      <td>0.842881</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.398354</td>\n",
              "      <td>0.376973</td>\n",
              "      <td>0.839434</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.379251</td>\n",
              "      <td>0.376215</td>\n",
              "      <td>0.840623</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.394695</td>\n",
              "      <td>0.378743</td>\n",
              "      <td>0.839910</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7GTTw1GMl2F",
        "colab_type": "code",
        "outputId": "928d927d-4915-42ff-bc2b-092b9cd2fec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3149
        }
      },
      "source": [
        "learn.fit_one_cycle(100, 1e-2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.653021</td>\n",
              "      <td>0.574230</td>\n",
              "      <td>0.776206</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.575820</td>\n",
              "      <td>0.501459</td>\n",
              "      <td>0.793083</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.532598</td>\n",
              "      <td>0.469748</td>\n",
              "      <td>0.798669</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.544697</td>\n",
              "      <td>0.468612</td>\n",
              "      <td>0.797837</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.532855</td>\n",
              "      <td>0.455535</td>\n",
              "      <td>0.805206</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.525249</td>\n",
              "      <td>0.456973</td>\n",
              "      <td>0.804136</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.510769</td>\n",
              "      <td>0.454591</td>\n",
              "      <td>0.806870</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.536015</td>\n",
              "      <td>0.437382</td>\n",
              "      <td>0.814476</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.509272</td>\n",
              "      <td>0.451316</td>\n",
              "      <td>0.809484</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.490977</td>\n",
              "      <td>0.438072</td>\n",
              "      <td>0.815427</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.522011</td>\n",
              "      <td>0.442791</td>\n",
              "      <td>0.807820</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.527490</td>\n",
              "      <td>0.440558</td>\n",
              "      <td>0.811386</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.488401</td>\n",
              "      <td>0.436384</td>\n",
              "      <td>0.810910</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.516407</td>\n",
              "      <td>0.425976</td>\n",
              "      <td>0.820418</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.521006</td>\n",
              "      <td>0.439500</td>\n",
              "      <td>0.811267</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.507632</td>\n",
              "      <td>0.443653</td>\n",
              "      <td>0.813525</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.508079</td>\n",
              "      <td>0.436748</td>\n",
              "      <td>0.818160</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.514807</td>\n",
              "      <td>0.435304</td>\n",
              "      <td>0.814714</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.499217</td>\n",
              "      <td>0.449733</td>\n",
              "      <td>0.805681</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.471220</td>\n",
              "      <td>0.436999</td>\n",
              "      <td>0.812218</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.506074</td>\n",
              "      <td>0.436914</td>\n",
              "      <td>0.812099</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.504349</td>\n",
              "      <td>0.459025</td>\n",
              "      <td>0.798431</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.467521</td>\n",
              "      <td>0.436713</td>\n",
              "      <td>0.810792</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.537397</td>\n",
              "      <td>0.430150</td>\n",
              "      <td>0.821488</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.487918</td>\n",
              "      <td>0.438573</td>\n",
              "      <td>0.812931</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.503639</td>\n",
              "      <td>0.428505</td>\n",
              "      <td>0.817447</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.492183</td>\n",
              "      <td>0.439734</td>\n",
              "      <td>0.813763</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.487099</td>\n",
              "      <td>0.431881</td>\n",
              "      <td>0.816496</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.487752</td>\n",
              "      <td>0.425781</td>\n",
              "      <td>0.813287</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.501724</td>\n",
              "      <td>0.442198</td>\n",
              "      <td>0.812218</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.511663</td>\n",
              "      <td>0.441839</td>\n",
              "      <td>0.811148</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.521871</td>\n",
              "      <td>0.437634</td>\n",
              "      <td>0.814476</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.476406</td>\n",
              "      <td>0.407043</td>\n",
              "      <td>0.830045</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.464298</td>\n",
              "      <td>0.406852</td>\n",
              "      <td>0.825648</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.480072</td>\n",
              "      <td>0.449302</td>\n",
              "      <td>0.809484</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.483135</td>\n",
              "      <td>0.420167</td>\n",
              "      <td>0.823746</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.518508</td>\n",
              "      <td>0.439484</td>\n",
              "      <td>0.809128</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.456846</td>\n",
              "      <td>0.404514</td>\n",
              "      <td>0.828500</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.457131</td>\n",
              "      <td>0.395620</td>\n",
              "      <td>0.830045</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.469439</td>\n",
              "      <td>0.431563</td>\n",
              "      <td>0.820418</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.465601</td>\n",
              "      <td>0.417436</td>\n",
              "      <td>0.827312</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.466437</td>\n",
              "      <td>0.407774</td>\n",
              "      <td>0.826123</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.489427</td>\n",
              "      <td>0.409308</td>\n",
              "      <td>0.831234</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.457066</td>\n",
              "      <td>0.408308</td>\n",
              "      <td>0.822676</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.465491</td>\n",
              "      <td>0.419610</td>\n",
              "      <td>0.817923</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.472315</td>\n",
              "      <td>0.409442</td>\n",
              "      <td>0.824340</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.458907</td>\n",
              "      <td>0.417864</td>\n",
              "      <td>0.828144</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.449060</td>\n",
              "      <td>0.418953</td>\n",
              "      <td>0.826717</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.455520</td>\n",
              "      <td>0.416523</td>\n",
              "      <td>0.823271</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.447787</td>\n",
              "      <td>0.415186</td>\n",
              "      <td>0.823271</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.447916</td>\n",
              "      <td>0.398967</td>\n",
              "      <td>0.834799</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.478179</td>\n",
              "      <td>0.413354</td>\n",
              "      <td>0.833254</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.434690</td>\n",
              "      <td>0.402329</td>\n",
              "      <td>0.832898</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.455278</td>\n",
              "      <td>0.409993</td>\n",
              "      <td>0.829213</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.436556</td>\n",
              "      <td>0.393451</td>\n",
              "      <td>0.833016</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.439583</td>\n",
              "      <td>0.388963</td>\n",
              "      <td>0.837652</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.432050</td>\n",
              "      <td>0.376962</td>\n",
              "      <td>0.843119</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.430590</td>\n",
              "      <td>0.384641</td>\n",
              "      <td>0.839553</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.444057</td>\n",
              "      <td>0.393456</td>\n",
              "      <td>0.836582</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.430182</td>\n",
              "      <td>0.385120</td>\n",
              "      <td>0.839553</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.452199</td>\n",
              "      <td>0.393483</td>\n",
              "      <td>0.836463</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.410522</td>\n",
              "      <td>0.373114</td>\n",
              "      <td>0.843119</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.460981</td>\n",
              "      <td>0.388008</td>\n",
              "      <td>0.834918</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.408000</td>\n",
              "      <td>0.381320</td>\n",
              "      <td>0.838483</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.441713</td>\n",
              "      <td>0.378041</td>\n",
              "      <td>0.839315</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.403117</td>\n",
              "      <td>0.373992</td>\n",
              "      <td>0.841217</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.395084</td>\n",
              "      <td>0.379297</td>\n",
              "      <td>0.841692</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.389875</td>\n",
              "      <td>0.383324</td>\n",
              "      <td>0.842287</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.411945</td>\n",
              "      <td>0.394422</td>\n",
              "      <td>0.833254</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.390047</td>\n",
              "      <td>0.388472</td>\n",
              "      <td>0.838840</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.405904</td>\n",
              "      <td>0.369957</td>\n",
              "      <td>0.847635</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.382354</td>\n",
              "      <td>0.380189</td>\n",
              "      <td>0.838008</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.396742</td>\n",
              "      <td>0.365343</td>\n",
              "      <td>0.846803</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.403185</td>\n",
              "      <td>0.372447</td>\n",
              "      <td>0.844783</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.410202</td>\n",
              "      <td>0.369852</td>\n",
              "      <td>0.847754</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.388575</td>\n",
              "      <td>0.377416</td>\n",
              "      <td>0.841811</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.400425</td>\n",
              "      <td>0.373517</td>\n",
              "      <td>0.842524</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.363888</td>\n",
              "      <td>0.384575</td>\n",
              "      <td>0.844188</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.396280</td>\n",
              "      <td>0.373037</td>\n",
              "      <td>0.844426</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.384447</td>\n",
              "      <td>0.359496</td>\n",
              "      <td>0.852745</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.373266</td>\n",
              "      <td>0.356461</td>\n",
              "      <td>0.850963</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.382070</td>\n",
              "      <td>0.360212</td>\n",
              "      <td>0.850487</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.375894</td>\n",
              "      <td>0.377506</td>\n",
              "      <td>0.842049</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.361993</td>\n",
              "      <td>0.355999</td>\n",
              "      <td>0.851913</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.358062</td>\n",
              "      <td>0.366142</td>\n",
              "      <td>0.847754</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.341124</td>\n",
              "      <td>0.352530</td>\n",
              "      <td>0.854290</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.397669</td>\n",
              "      <td>0.361291</td>\n",
              "      <td>0.847991</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.375779</td>\n",
              "      <td>0.358973</td>\n",
              "      <td>0.850725</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.359185</td>\n",
              "      <td>0.358438</td>\n",
              "      <td>0.853102</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.362610</td>\n",
              "      <td>0.354225</td>\n",
              "      <td>0.855360</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.372519</td>\n",
              "      <td>0.354532</td>\n",
              "      <td>0.852627</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.368131</td>\n",
              "      <td>0.355318</td>\n",
              "      <td>0.853459</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.352424</td>\n",
              "      <td>0.355428</td>\n",
              "      <td>0.853459</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.355302</td>\n",
              "      <td>0.356011</td>\n",
              "      <td>0.850487</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.353374</td>\n",
              "      <td>0.354498</td>\n",
              "      <td>0.854528</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.375212</td>\n",
              "      <td>0.357012</td>\n",
              "      <td>0.848110</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.374279</td>\n",
              "      <td>0.354898</td>\n",
              "      <td>0.852508</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.369498</td>\n",
              "      <td>0.353172</td>\n",
              "      <td>0.854885</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.352362</td>\n",
              "      <td>0.352955</td>\n",
              "      <td>0.853102</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.386029</td>\n",
              "      <td>0.356249</td>\n",
              "      <td>0.852627</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUxbgHOTJiQz",
        "colab_type": "code",
        "outputId": "e18d2bdd-b35e-40f8-fde3-c87edd063cd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        }
      },
      "source": [
        "#Oversampling\n",
        "\n",
        "learn.fit_one_cycle(20, 1e-2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.566685</td>\n",
              "      <td>0.517436</td>\n",
              "      <td>0.812689</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.542097</td>\n",
              "      <td>0.527252</td>\n",
              "      <td>0.810773</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.505091</td>\n",
              "      <td>0.498490</td>\n",
              "      <td>0.815614</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.532351</td>\n",
              "      <td>0.505338</td>\n",
              "      <td>0.820960</td>\n",
              "      <td>00:13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.517055</td>\n",
              "      <td>0.495234</td>\n",
              "      <td>0.819044</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.513420</td>\n",
              "      <td>0.522818</td>\n",
              "      <td>0.811580</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.539227</td>\n",
              "      <td>0.505133</td>\n",
              "      <td>0.811983</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.507165</td>\n",
              "      <td>0.496337</td>\n",
              "      <td>0.818439</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.523650</td>\n",
              "      <td>0.505724</td>\n",
              "      <td>0.811882</td>\n",
              "      <td>00:13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.492227</td>\n",
              "      <td>0.489392</td>\n",
              "      <td>0.819649</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.511125</td>\n",
              "      <td>0.489315</td>\n",
              "      <td>0.815917</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.524485</td>\n",
              "      <td>0.482800</td>\n",
              "      <td>0.819145</td>\n",
              "      <td>00:13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.488206</td>\n",
              "      <td>0.456656</td>\n",
              "      <td>0.833064</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.473052</td>\n",
              "      <td>0.495941</td>\n",
              "      <td>0.820557</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.471741</td>\n",
              "      <td>0.464061</td>\n",
              "      <td>0.828525</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.463832</td>\n",
              "      <td>0.472611</td>\n",
              "      <td>0.826306</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.491346</td>\n",
              "      <td>0.471099</td>\n",
              "      <td>0.826004</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.454062</td>\n",
              "      <td>0.490531</td>\n",
              "      <td>0.825197</td>\n",
              "      <td>00:13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.440299</td>\n",
              "      <td>0.464269</td>\n",
              "      <td>0.829635</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.467103</td>\n",
              "      <td>0.473375</td>\n",
              "      <td>0.826609</td>\n",
              "      <td>00:13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTt0g8VX_lF7",
        "colab_type": "code",
        "outputId": "c416f68c-e088-4dc2-daae-91998b760274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        }
      },
      "source": [
        "# General without preprocessing\n",
        "\n",
        "learn.fit_one_cycle(20, 1e-2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.691303</td>\n",
              "      <td>0.558598</td>\n",
              "      <td>0.782615</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.644233</td>\n",
              "      <td>0.541855</td>\n",
              "      <td>0.775333</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.642364</td>\n",
              "      <td>0.536564</td>\n",
              "      <td>0.774558</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.631345</td>\n",
              "      <td>0.562451</td>\n",
              "      <td>0.758289</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.612675</td>\n",
              "      <td>0.504514</td>\n",
              "      <td>0.789898</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.624449</td>\n",
              "      <td>0.503726</td>\n",
              "      <td>0.786179</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.616386</td>\n",
              "      <td>0.508229</td>\n",
              "      <td>0.784475</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.616527</td>\n",
              "      <td>0.520840</td>\n",
              "      <td>0.781531</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.618587</td>\n",
              "      <td>0.506106</td>\n",
              "      <td>0.791602</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.602859</td>\n",
              "      <td>0.514996</td>\n",
              "      <td>0.782461</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.582892</td>\n",
              "      <td>0.488729</td>\n",
              "      <td>0.793926</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.592667</td>\n",
              "      <td>0.495079</td>\n",
              "      <td>0.796405</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.582448</td>\n",
              "      <td>0.481035</td>\n",
              "      <td>0.795011</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.555630</td>\n",
              "      <td>0.479215</td>\n",
              "      <td>0.797335</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.564672</td>\n",
              "      <td>0.494094</td>\n",
              "      <td>0.788813</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.577038</td>\n",
              "      <td>0.473315</td>\n",
              "      <td>0.798110</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.558971</td>\n",
              "      <td>0.471739</td>\n",
              "      <td>0.796095</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.546905</td>\n",
              "      <td>0.477003</td>\n",
              "      <td>0.799039</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.558202</td>\n",
              "      <td>0.484947</td>\n",
              "      <td>0.794391</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.551090</td>\n",
              "      <td>0.466804</td>\n",
              "      <td>0.797955</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCm_vIe2AMv7",
        "colab_type": "code",
        "outputId": "1d5ceea2-b490-4409-c9c5-b8b5c76622ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        }
      },
      "source": [
        "#drop_mul = 0.3\n",
        "learn.fit_one_cycle(20, 1e-2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.630025</td>\n",
              "      <td>0.577467</td>\n",
              "      <td>0.767293</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.579487</td>\n",
              "      <td>0.528732</td>\n",
              "      <td>0.760043</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.572823</td>\n",
              "      <td>0.497073</td>\n",
              "      <td>0.788210</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.559528</td>\n",
              "      <td>0.490734</td>\n",
              "      <td>0.788448</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.562241</td>\n",
              "      <td>0.468228</td>\n",
              "      <td>0.798431</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.545786</td>\n",
              "      <td>0.467924</td>\n",
              "      <td>0.800333</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.537161</td>\n",
              "      <td>0.477581</td>\n",
              "      <td>0.795222</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.532120</td>\n",
              "      <td>0.457542</td>\n",
              "      <td>0.803066</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.535088</td>\n",
              "      <td>0.454426</td>\n",
              "      <td>0.805206</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.515849</td>\n",
              "      <td>0.477173</td>\n",
              "      <td>0.792370</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.514348</td>\n",
              "      <td>0.442645</td>\n",
              "      <td>0.804968</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.468854</td>\n",
              "      <td>0.434961</td>\n",
              "      <td>0.812574</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.487502</td>\n",
              "      <td>0.442431</td>\n",
              "      <td>0.811980</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.463051</td>\n",
              "      <td>0.432351</td>\n",
              "      <td>0.815308</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.464223</td>\n",
              "      <td>0.418073</td>\n",
              "      <td>0.817447</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.430841</td>\n",
              "      <td>0.405406</td>\n",
              "      <td>0.825885</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.470666</td>\n",
              "      <td>0.411314</td>\n",
              "      <td>0.822439</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.468732</td>\n",
              "      <td>0.402779</td>\n",
              "      <td>0.825529</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.455271</td>\n",
              "      <td>0.399186</td>\n",
              "      <td>0.827668</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.424595</td>\n",
              "      <td>0.401959</td>\n",
              "      <td>0.827312</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUXsbdykNkHj",
        "colab_type": "code",
        "outputId": "1bba7758-6870-4e06-83d9-134c962c7ea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1599
        }
      },
      "source": [
        "learn.fit_one_cycle(50, 1e-2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.617558</td>\n",
              "      <td>0.548942</td>\n",
              "      <td>0.778227</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.573675</td>\n",
              "      <td>0.502053</td>\n",
              "      <td>0.786546</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.574079</td>\n",
              "      <td>0.487434</td>\n",
              "      <td>0.790349</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.543775</td>\n",
              "      <td>0.478375</td>\n",
              "      <td>0.793202</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.538428</td>\n",
              "      <td>0.483028</td>\n",
              "      <td>0.791419</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.544638</td>\n",
              "      <td>0.464327</td>\n",
              "      <td>0.799976</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.514850</td>\n",
              "      <td>0.461437</td>\n",
              "      <td>0.800808</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.528654</td>\n",
              "      <td>0.447244</td>\n",
              "      <td>0.805443</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.548981</td>\n",
              "      <td>0.446111</td>\n",
              "      <td>0.813525</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.512610</td>\n",
              "      <td>0.452734</td>\n",
              "      <td>0.800570</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.537795</td>\n",
              "      <td>0.447569</td>\n",
              "      <td>0.812218</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.504305</td>\n",
              "      <td>0.447601</td>\n",
              "      <td>0.809841</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.534867</td>\n",
              "      <td>0.437209</td>\n",
              "      <td>0.808177</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.510802</td>\n",
              "      <td>0.449150</td>\n",
              "      <td>0.805443</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.499259</td>\n",
              "      <td>0.449416</td>\n",
              "      <td>0.812455</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.527378</td>\n",
              "      <td>0.459724</td>\n",
              "      <td>0.806870</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.515020</td>\n",
              "      <td>0.455600</td>\n",
              "      <td>0.802947</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.487768</td>\n",
              "      <td>0.443824</td>\n",
              "      <td>0.815664</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.508330</td>\n",
              "      <td>0.430181</td>\n",
              "      <td>0.815783</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.487338</td>\n",
              "      <td>0.446189</td>\n",
              "      <td>0.815189</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.493226</td>\n",
              "      <td>0.423279</td>\n",
              "      <td>0.820894</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.488874</td>\n",
              "      <td>0.415179</td>\n",
              "      <td>0.820656</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.490111</td>\n",
              "      <td>0.422089</td>\n",
              "      <td>0.823508</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.470840</td>\n",
              "      <td>0.413079</td>\n",
              "      <td>0.827668</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.468994</td>\n",
              "      <td>0.417568</td>\n",
              "      <td>0.825053</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.489458</td>\n",
              "      <td>0.426330</td>\n",
              "      <td>0.816259</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.456556</td>\n",
              "      <td>0.411195</td>\n",
              "      <td>0.828381</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.478885</td>\n",
              "      <td>0.427645</td>\n",
              "      <td>0.828381</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.465644</td>\n",
              "      <td>0.406603</td>\n",
              "      <td>0.823746</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.464089</td>\n",
              "      <td>0.397782</td>\n",
              "      <td>0.835750</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.428836</td>\n",
              "      <td>0.395608</td>\n",
              "      <td>0.833848</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.423849</td>\n",
              "      <td>0.393629</td>\n",
              "      <td>0.831471</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.455810</td>\n",
              "      <td>0.387938</td>\n",
              "      <td>0.833848</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.436241</td>\n",
              "      <td>0.394419</td>\n",
              "      <td>0.837533</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.424991</td>\n",
              "      <td>0.375618</td>\n",
              "      <td>0.846090</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.441158</td>\n",
              "      <td>0.385677</td>\n",
              "      <td>0.838959</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.405180</td>\n",
              "      <td>0.380719</td>\n",
              "      <td>0.840742</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.420560</td>\n",
              "      <td>0.376009</td>\n",
              "      <td>0.841217</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.399202</td>\n",
              "      <td>0.367989</td>\n",
              "      <td>0.847278</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.400268</td>\n",
              "      <td>0.371208</td>\n",
              "      <td>0.843713</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.423345</td>\n",
              "      <td>0.371006</td>\n",
              "      <td>0.844901</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.385291</td>\n",
              "      <td>0.368933</td>\n",
              "      <td>0.847278</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.386227</td>\n",
              "      <td>0.373468</td>\n",
              "      <td>0.845496</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.395614</td>\n",
              "      <td>0.363256</td>\n",
              "      <td>0.850725</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.387247</td>\n",
              "      <td>0.368407</td>\n",
              "      <td>0.844426</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.378241</td>\n",
              "      <td>0.364144</td>\n",
              "      <td>0.847160</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.370011</td>\n",
              "      <td>0.361876</td>\n",
              "      <td>0.850368</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.423324</td>\n",
              "      <td>0.361641</td>\n",
              "      <td>0.848229</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.402322</td>\n",
              "      <td>0.362497</td>\n",
              "      <td>0.847991</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.391550</td>\n",
              "      <td>0.357801</td>\n",
              "      <td>0.852745</td>\n",
              "      <td>00:12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NADJEzb_kEn",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFML3zMB-20V",
        "colab_type": "code",
        "outputId": "4638b57d-e518-4df9-d982-67a61a4f69fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "preds, targets = learn.get_preds()\n",
        "\n",
        "predictions = np.argmax(preds, axis = 1)\n",
        "pd.crosstab(predictions, targets)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3658</td>\n",
              "      <td>170</td>\n",
              "      <td>407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>284</td>\n",
              "      <td>1821</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>472</td>\n",
              "      <td>9</td>\n",
              "      <td>1588</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0     0     1     2\n",
              "row_0                  \n",
              "0      3658   170   407\n",
              "1       284  1821     5\n",
              "2       472     9  1588"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_hfwpOX_DVA",
        "colab_type": "code",
        "outputId": "72656e18-bc1f-430a-fe19-9151bf5e32bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "preds, targets = learn.get_preds()\n",
        "\n",
        "predictions = np.argmax(preds, axis = 1)\n",
        "pd.crosstab(predictions, targets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3922</td>\n",
              "      <td>272</td>\n",
              "      <td>629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>218</td>\n",
              "      <td>496</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>273</td>\n",
              "      <td>6</td>\n",
              "      <td>635</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0     0    1    2\n",
              "row_0                \n",
              "0      3922  272  629\n",
              "1       218  496    3\n",
              "2       273    6  635"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQgNmU77AZ7o",
        "colab_type": "code",
        "outputId": "6db2a993-ebcf-4417-a914-a19aba4dee92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "preds, targets = learn.get_preds()\n",
        "\n",
        "predictions = np.argmax(preds, axis = 1)\n",
        "pd.crosstab(predictions, targets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3928</td>\n",
              "      <td>188</td>\n",
              "      <td>620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>298</td>\n",
              "      <td>584</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>187</td>\n",
              "      <td>2</td>\n",
              "      <td>638</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0     0    1    2\n",
              "row_0                \n",
              "0      3928  188  620\n",
              "1       298  584    9\n",
              "2       187    2  638"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycetrH7UH3IG",
        "colab_type": "code",
        "outputId": "466a9aac-5e5f-4107-dd70-72a12d54ed0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "targets"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0,  ..., 2, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45XN9vMJH5WP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDHGCUeZIHgW",
        "colab_type": "code",
        "outputId": "ee0d57ae-80c3-4616-c043-986f0c80fbd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "print (tf.metrics.accuracy(\n",
        "    targets,\n",
        "    predictions,\n",
        "    weights=None,\n",
        "    metrics_collections=None,\n",
        "    updates_collections=None,\n",
        "    name=None\n",
        "))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor 'accuracy_1/value:0' shape=() dtype=float32>, <tf.Tensor 'accuracy_1/update_op:0' shape=() dtype=float32>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQgRMXo7IJW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb79LWOMIZ7Q",
        "colab_type": "code",
        "outputId": "7eefd079-7736-4e91-872c-34b66c574f0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# General\n",
        "accuracy_score(targets, predictions)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.83990967435227"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJsCvUo3Id6Y",
        "colab_type": "code",
        "outputId": "71a5cffa-74f4-4fa0-cde5-286cdf8dc552",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Oversamping\n",
        "preds, targets = learn.get_preds()\n",
        "\n",
        "predictions = np.argmax(preds, axis = 1)\n",
        "pd.crosstab(predictions, targets)\n",
        "accuracy_score(targets, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8233895887806038"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDPFiB5TJnJv",
        "colab_type": "code",
        "outputId": "41befd8b-4436-4041-866d-9f83608654cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "pd.crosstab(predictions, targets)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3623</td>\n",
              "      <td>197</td>\n",
              "      <td>712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>465</td>\n",
              "      <td>1792</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>326</td>\n",
              "      <td>11</td>\n",
              "      <td>1248</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0     0     1     2\n",
              "row_0                  \n",
              "0      3623   197   712\n",
              "1       465  1792    40\n",
              "2       326    11  1248"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmRR3WwrKshq",
        "colab_type": "code",
        "outputId": "4074be04-9c3c-42df-d9f6-266cd6b688e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Drop Multitude changed\n",
        "\n",
        "preds, targets = learn.get_preds()\n",
        "\n",
        "predictions = np.argmax(preds, axis = 1)\n",
        "print (pd.crosstab(predictions, targets))\n",
        "print (accuracy_score(targets, predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "col_0     0     1     2\n",
            "row_0                  \n",
            "0      3726   232   512\n",
            "1       324  1763    16\n",
            "2       364     5  1472\n",
            "0.8273116234846684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8quSHTdMX5F",
        "colab_type": "code",
        "outputId": "ca306faf-c7ca-41a7-d133-4407ca0d45b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Drop Multitude changed 50 epochs\n",
        "\n",
        "preds, targets = learn.get_preds()\n",
        "\n",
        "predictions = np.argmax(preds, axis = 1)\n",
        "print (pd.crosstab(predictions, targets))\n",
        "print (accuracy_score(targets, predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "col_0     0     1     2\n",
            "row_0                  \n",
            "0      3750   163   403\n",
            "1       312  1833     5\n",
            "2       352     4  1592\n",
            "0.8527454242928453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEcstVp0OS12",
        "colab_type": "code",
        "outputId": "376f995f-85d4-4542-afdd-fdac149a8aaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Drop Multitude changed 100 epochs\n",
        "\n",
        "preds, targets = learn_model.get_preds()\n",
        "\n",
        "predictions = np.argmax(preds, axis = 1)\n",
        "print (pd.crosstab(predictions, targets))\n",
        "print (accuracy_score(targets, predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "col_0     0     1     2\n",
            "row_0                  \n",
            "0      3800   227   630\n",
            "1       319  1766     8\n",
            "2       295     7  1362\n",
            "0.8233895887806038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XXcX6Ucna3l",
        "colab_type": "text"
      },
      "source": [
        "# Saving ULMFit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBb2EEcyLsD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_instance = learn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxpn3phCM8Ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_instance.save('model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyDOlduTNCcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_model = text_classifier_learner(data_clas,arch= AWD_LSTM, drop_mult=0.3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_cz9ZthnHWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_clas.save(\"data_save.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsthZLK9NQnU",
        "colab_type": "code",
        "outputId": "eae69200-5146-43f5-b3c0-b02f3878633d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data_clas = load_data('./')\n",
        "\n",
        "learn_model = text_classifier_learner(data_clas,arch= AWD_LSTM, drop_mult=0.3)\n",
        "learn_model.load(\"model\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (12619 items)\n",
              "x: TextList\n",
              "xxbos @tintinenameriq @maxblumenthal @campagnebds xxmaj shows how stupid you 've become . xxmaj only xxmaj muslim terrorists actually attack talks .,xxbos @98halima @johnnygjokaj @bilalighumman @cdnkhadija @rfrankh53 xxup isis wants to control the world because xxmaj mohammed wanted to control it .,xxbos xxmaj he is not all there . xxmaj and i guess he keeps forgetting that the dog just went outside and ug xxrep 4 h i want to sleep .,xxbos xxmaj oh $ xxunk , xxmaj psycho xxmaj annie is back . > xxunk # xxup mkr,xxbos xxmaj the sexist litmus test is if you call out a lvl 2 sexist comment & & the dood comes back with lvl 10 guns ablazing , \" pussy \" & & \" cunt \" & & \" bitch \"\n",
              "y: CategoryList\n",
              "1,1,0,0,2\n",
              "Path: .;\n",
              "\n",
              "Valid: LabelList (8414 items)\n",
              "x: TextList\n",
              "xxbos xxup rt http : / / t.co / xxunk xxunk xxmaj call me sexist but men will always be superior drivers,xxbos xxup rt @shermertron : xxup @mt8_9 xxunk xxmaj the xxmaj democratic xxmaj people 's xxmaj xxunk of xxmaj korea must be a wonderful place , right ? xxmaj look up the words ! ! !,xxbos # mkr,xxbos xxunk and what a xxunk ass you are . xxmaj kindly go fuck yourself . :),xxbos @farooqsumar @nafeezahmed @maxblumenthal xxmaj in fact the pedophile prophet clearly stated his intention in xxmaj hadiths . http : / / t.co / xxunk\n",
              "y: CategoryList\n",
              "2,2,0,0,1\n",
              "Path: .;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(9576, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(9576, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.12, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=3, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f9fc643ad08>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
              "learn: RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (12619 items)\n",
              "x: TextList\n",
              "xxbos @tintinenameriq @maxblumenthal @campagnebds xxmaj shows how stupid you 've become . xxmaj only xxmaj muslim terrorists actually attack talks .,xxbos @98halima @johnnygjokaj @bilalighumman @cdnkhadija @rfrankh53 xxup isis wants to control the world because xxmaj mohammed wanted to control it .,xxbos xxmaj he is not all there . xxmaj and i guess he keeps forgetting that the dog just went outside and ug xxrep 4 h i want to sleep .,xxbos xxmaj oh $ xxunk , xxmaj psycho xxmaj annie is back . > xxunk # xxup mkr,xxbos xxmaj the sexist litmus test is if you call out a lvl 2 sexist comment & & the dood comes back with lvl 10 guns ablazing , \" pussy \" & & \" cunt \" & & \" bitch \"\n",
              "y: CategoryList\n",
              "1,1,0,0,2\n",
              "Path: .;\n",
              "\n",
              "Valid: LabelList (8414 items)\n",
              "x: TextList\n",
              "xxbos xxup rt http : / / t.co / xxunk xxunk xxmaj call me sexist but men will always be superior drivers,xxbos xxup rt @shermertron : xxup @mt8_9 xxunk xxmaj the xxmaj democratic xxmaj people 's xxmaj xxunk of xxmaj korea must be a wonderful place , right ? xxmaj look up the words ! ! !,xxbos # mkr,xxbos xxunk and what a xxunk ass you are . xxmaj kindly go fuck yourself . :),xxbos @farooqsumar @nafeezahmed @maxblumenthal xxmaj in fact the pedophile prophet clearly stated his intention in xxmaj hadiths . http : / / t.co / xxunk\n",
              "y: CategoryList\n",
              "2,2,0,0,1\n",
              "Path: .;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(9576, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(9576, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.12, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=3, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f9fc643ad08>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
              "  (0): Embedding(9576, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(9576, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.12, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=3, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False, cb_fns_registered=False)\n",
              "alpha: 2.0\n",
              "beta: 1.0], layer_groups=[Sequential(\n",
              "  (0): Embedding(9576, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(9576, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.12, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=3, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False, cb_fns_registered=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    }
  ]
}